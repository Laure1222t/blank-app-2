import streamlit as st
import fitz  # PyMuPDF
import re
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import docx
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
import os
import tempfile
import time

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ”¿ç­–æ–‡ä»¶æ¯”å¯¹åˆ†æå·¥å…·",
    page_icon="ğŸ“œ",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ“œ ä¸­æ–‡æ”¿ç­–æ–‡ä»¶æ¯”å¯¹åˆ†æå·¥å…·")
st.markdown("ä¸Šä¼ ç›®æ ‡æ”¿ç­–æ–‡ä»¶å’Œå¾…æ¯”å¯¹æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è§£æå¹¶è¿›è¡Œæ¡æ¬¾æ¯”å¯¹ä¸åˆè§„æ€§åˆ†æ")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'target_doc' not in st.session_state:
    st.session_state.target_doc = None
if 'compare_doc' not in st.session_state:
    st.session_state.compare_doc = None
if 'analysis_result' not in st.session_state:
    st.session_state.analysis_result = None

# åŠ è½½Qwenæ¨¡å‹å’Œtokenizer
@st.cache_resource
def load_model():
    try:
        with st.spinner("æ­£åœ¨åŠ è½½Qwenå¤§æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
            tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen-7B-Chat", 
                device_map="auto", 
                trust_remote_code=True
            )
            model.eval()
            return tokenizer, model
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None

# è§£æPDFæ–‡ä»¶
def parse_pdf(file):
    try:
        doc = fitz.open(stream=file.read(), filetype="pdf")
        text = ""
        for page in doc:
            text += page.get_text()
        
        # ç®€å•çš„æ¡æ¬¾æå–ï¼Œå¯æ ¹æ®å®é™…éœ€æ±‚ä¼˜åŒ–
        clauses = []
        # åŒ¹é…ä»¥æ•°å­—åŠ ç‚¹å¼€å¤´çš„æ¡æ¬¾ï¼ˆå¦‚ 1. 2.1 ç­‰ï¼‰
        pattern = re.compile(r'(\d+\.\s+|\d+\.\d+\s+).+?(?=\d+\.\s+|\d+\.\d+\s+|$)', re.DOTALL)
        matches = pattern.findall(text)
        
        if matches:
            for match in matches:
                clause_text = match[0] + match[1].strip()
                clauses.append(clause_text)
        else:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ¡æ¬¾æ ¼å¼ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            clauses = paragraphs[:20]  # å–å‰20ä¸ªæ®µè½
        
        return clauses
    except Exception as e:
        st.error(f"PDFè§£æå¤±è´¥: {str(e)}")
        return []

# ä½¿ç”¨Qwenæ¨¡å‹è¿›è¡Œåˆè§„æ€§åˆ†æ
def analyze_compliance(target_clauses, compare_clauses, tokenizer, model):
    if not tokenizer or not model:
        return "æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ"
    
    try:
        with st.spinner("æ­£åœ¨è¿›è¡Œæ¡æ¬¾æ¯”å¯¹å’Œåˆè§„æ€§åˆ†æï¼Œè¯·ç¨å€™..."):
            # æ„å»ºåˆ†ææç¤º
            prompt = """
            è¯·æ¯”å¯¹ä»¥ä¸‹ä¸¤ä»½æ”¿ç­–æ–‡ä»¶çš„æ¡æ¬¾ï¼Œè¿›è¡Œåˆè§„æ€§åˆ†æã€‚
            åˆ†æè¦æ±‚ï¼š
            1. å°½é‡è¦†ç›–æ‰€æœ‰æ¡æ¬¾ï¼Œç¡®ä¿åˆ†æå…¨é¢
            2. é‡ç‚¹è¿›è¡Œåˆè§„æ€§åˆ†æï¼Œåˆ¤æ–­ä¸åŒä¹‹å¤„æ˜¯å¦å­˜åœ¨å†²çªï¼Œè€Œä¸ä»…ä»…æ˜¯æŒ‡å‡ºå·®å¼‚
            3. å¯¹äºç›¸åŒæˆ–ä¸€è‡´çš„æ¡æ¬¾å¯ä»¥ç®€è¦è¯´æ˜
            4. å¯¹äºå­˜åœ¨å·®å¼‚çš„æ¡æ¬¾ï¼Œè¯¦ç»†åˆ†ææ˜¯å¦å­˜åœ¨åˆè§„æ€§å†²çªï¼Œä»¥åŠå¯èƒ½çš„å½±å“
            
            ç›®æ ‡æ”¿ç­–æ–‡ä»¶æ¡æ¬¾ï¼š
            {}
            
            å¾…æ¯”å¯¹æ–‡ä»¶æ¡æ¬¾ï¼š
            {}
            
            è¯·ä»¥ç»“æ„åŒ–çš„æ–¹å¼è¾“å‡ºåˆ†æç»“æœï¼ŒåŒ…æ‹¬æ¡æ¬¾å¯¹åº”å…³ç³»ã€å·®å¼‚ç‚¹å’Œåˆè§„æ€§åˆ†æã€‚
            """.format("\n".join(target_clauses[:10]), "\n".join(compare_clauses[:10]))
            
            # è°ƒç”¨æ¨¡å‹
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=1500,
                temperature=0.7,
                top_p=0.9
            )
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æ¨¡å‹å›ç­”ï¼ˆå»é™¤æç¤ºéƒ¨åˆ†ï¼‰
            result_start = result.find("ç›®æ ‡æ”¿ç­–æ–‡ä»¶æ¡æ¬¾ï¼š")
            if result_start != -1:
                result = result[result_start:]
                
            return result
    except Exception as e:
        st.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        return f"åˆ†æå¤±è´¥: {str(e)}"

# ç”ŸæˆWordæ–‡æ¡£
def generate_word_document(analysis_result, target_filename, compare_filename):
    try:
        doc = docx.Document()
        
        # æ·»åŠ æ ‡é¢˜
        title = doc.add_heading("æ”¿ç­–æ–‡ä»¶åˆè§„æ€§åˆ†ææŠ¥å‘Š", 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
        doc.add_paragraph(f"ç›®æ ‡æ”¿ç­–æ–‡ä»¶: {target_filename}")
        doc.add_paragraph(f"å¾…æ¯”å¯¹æ–‡ä»¶: {compare_filename}")
        doc.add_paragraph(f"åˆ†ææ—¥æœŸ: {time.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        doc.add_paragraph("")
        
        # æ·»åŠ åˆ†æç»“æœ
        doc.add_heading("åˆ†æç»“æœ", level=1)
        
        # ç®€å•å¤„ç†åˆ†æç»“æœï¼ŒæŒ‰æ¢è¡Œåˆ†å‰²
        paragraphs = analysis_result.split('\n')
        for para in paragraphs:
            if para.strip():
                p = doc.add_paragraph(para.strip())
                p.paragraph_format.space_after = Pt(12)
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:
            doc.save(tmp.name)
            return tmp.name
    except Exception as e:
        st.error(f"ç”ŸæˆWordæ–‡æ¡£å¤±è´¥: {str(e)}")
        return None

# ä¸»ç•Œé¢å¸ƒå±€
col1, col2 = st.columns(2)

with col1:
    st.subheader("ç›®æ ‡æ”¿ç­–æ–‡ä»¶ (å·¦ä¾§)")
    target_file = st.file_uploader("ä¸Šä¼ ç›®æ ‡æ”¿ç­–æ–‡ä»¶ (PDF)", type="pdf", key="target")
    
    if target_file:
        st.session_state.target_doc = parse_pdf(target_file)
        st.success(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œæå–åˆ° {len(st.session_state.target_doc)} æ¡æ¡æ¬¾")
        
        with st.expander("æŸ¥çœ‹æå–çš„æ¡æ¬¾"):
            for i, clause in enumerate(st.session_state.target_doc[:10]):  # åªæ˜¾ç¤ºå‰10æ¡
                st.write(f"æ¡æ¬¾ {i+1}: {clause[:100]}...")

with col2:
    st.subheader("å¾…æ¯”å¯¹æ–‡ä»¶ (å³ä¾§)")
    compare_file = st.file_uploader("ä¸Šä¼ å¾…æ¯”å¯¹æ–‡ä»¶ (PDF)", type="pdf", key="compare")
    
    if compare_file:
        st.session_state.compare_doc = parse_pdf(compare_file)
        st.success(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œæå–åˆ° {len(st.session_state.compare_doc)} æ¡æ¡æ¬¾")
        
        with st.expander("æŸ¥çœ‹æå–çš„æ¡æ¬¾"):
            for i, clause in enumerate(st.session_state.compare_doc[:10]):  # åªæ˜¾ç¤ºå‰10æ¡
                st.write(f"æ¡æ¬¾ {i+1}: {clause[:100]}...")

# åˆ†ææŒ‰é’®
if st.button("å¼€å§‹æ¯”å¯¹ä¸åˆè§„æ€§åˆ†æ"):
    if not st.session_state.target_doc or not st.session_state.compare_doc:
        st.warning("è¯·å…ˆä¸Šä¼ å¹¶è§£æä¸¤ä»½æ–‡ä»¶")
    else:
        # åŠ è½½æ¨¡å‹
        tokenizer, model = load_model()
        if tokenizer and model:
            # è¿›è¡Œåˆ†æ
            st.session_state.analysis_result = analyze_compliance(
                st.session_state.target_doc, 
                st.session_state.compare_doc,
                tokenizer,
                model
            )

# æ˜¾ç¤ºåˆ†æç»“æœ
if st.session_state.analysis_result:
    st.subheader("ğŸ“Š åˆè§„æ€§åˆ†æç»“æœ")
    st.text_area("", st.session_state.analysis_result, height=400)
    
    # ç”Ÿæˆå¹¶ä¸‹è½½Wordæ–‡æ¡£
    if target_file and compare_file:
        word_file = generate_word_document(
            st.session_state.analysis_result,
            target_file.name,
            compare_file.name
        )
        
        if word_file:
            with open(word_file, "rb") as f:
                st.download_button(
                    label="ä¸‹è½½åˆ†ææŠ¥å‘Š (Word)",
                    data=f,
                    file_name=f"æ”¿ç­–æ–‡ä»¶åˆè§„æ€§åˆ†ææŠ¥å‘Š_{time.strftime('%Y%m%d')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(word_file)

# é¡µè„šä¿¡æ¯
st.markdown("---")
st.markdown("å·¥å…·è¯´æ˜ï¼šæœ¬å·¥å…·ç”¨äºæ”¿ç­–æ–‡ä»¶çš„æ¡æ¬¾æ¯”å¯¹ä¸åˆè§„æ€§åˆ†æï¼Œç»“æœä»…ä¾›å‚è€ƒã€‚")
