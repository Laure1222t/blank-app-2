import streamlit as st
import docx
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
import re
import os
import tempfile
from datetime import datetime
from dashscope import Generation
import json
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import numpy as np
import io

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ“„ æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…· (ä¼˜åŒ–ç‰ˆ)")
st.write("ä¸Šä¼ åŸºå‡†æ–‡ä»¶å’Œå¾…æ¯”è¾ƒæ–‡ä»¶ï¼Œç³»ç»Ÿå°†ä¼˜å…ˆé€šè¿‡æ–‡æœ¬æå–åˆ†æï¼Œå¿…è¦æ—¶ä½¿ç”¨OCRè¯†åˆ«å›¾åƒå†…å®¹ã€‚")

# æ£€æŸ¥Tesseractæ˜¯å¦å®‰è£…
def check_tesseract_installation():
    try:
        # å°è¯•è¿è¡ŒTesseractç‰ˆæœ¬æ£€æŸ¥
        pytesseract.get_tesseract_version()
        return True
    except pytesseract.TesseractNotFoundError:
        return False

# æ£€æŸ¥TesseractçŠ¶æ€
tesseract_available = check_tesseract_installation()

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.subheader("é…ç½®é€‰é¡¹")
    
    # Qwen APIé…ç½®
    st.subheader("Qwenå¤§æ¨¡å‹é…ç½®")
    qwen_api_key = st.text_input("è¯·è¾“å…¥é˜¿é‡Œäº‘DashScope APIå¯†é’¥", type="password")
    if qwen_api_key:
        os.environ["DASHSCOPE_API_KEY"] = qwen_api_key
    
    # Tesseracté…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if not tesseract_available:
        st.warning("æœªæ£€æµ‹åˆ°Tesseract OCRå¼•æ“ï¼Œå›¾ç‰‡å‹PDFå°†æ— æ³•å¤„ç†")
        st.info("""
        å®‰è£…TesseractæŒ‡å—ï¼š
        1. ä¸‹è½½å®‰è£…åŒ…ï¼šhttps://github.com/UB-Mannheim/tesseract/wiki
        2. å®‰è£…æ—¶å‹¾é€‰ä¸­æ–‡è¯­è¨€åŒ…
        3. é…ç½®ç¯å¢ƒå˜é‡æˆ–åœ¨åº”ç”¨ä¸­æŒ‡å®šè·¯å¾„
        """)
    else:
        tesseract_path = st.text_input(
            "Tesseractå®‰è£…è·¯å¾„", 
            value=pytesseract.pytesseract.tesseract_cmd,
            help="é»˜è®¤è·¯å¾„: C:\\Program Files\\Tesseract-OCR\\tesseract.exe (Windows) æˆ– /usr/bin/tesseract (Linux)"
        )
        if tesseract_path:
            pytesseract.pytesseract.tesseract_cmd = tesseract_path

# è¾…åŠ©å‡½æ•°ï¼šä»docxæ–‡ä»¶ä¸­æå–æ–‡æœ¬
def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        if para.text.strip():  # åªæ·»åŠ éç©ºæ®µè½
            full_text.append(para.text)
    return '\n'.join(full_text)

# è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥PDFé¡µé¢æ˜¯å¦åŒ…å«å¯å¤åˆ¶æ–‡æœ¬
def has_selectable_text(page):
    text = page.get_text("text")
    # æ’é™¤ä»…åŒ…å«å°‘é‡å­—ç¬¦çš„æƒ…å†µï¼ˆå¯èƒ½æ˜¯é¡µçœ‰é¡µè„šï¼‰
    clean_text = text.strip()
    return len(clean_text) > 50  # è®¤ä¸º50ä¸ªå­—ç¬¦ä»¥ä¸Šæ˜¯æœ‰æ•ˆæ–‡æœ¬

# è¾…åŠ©å‡½æ•°ï¼šä»PDFä¸­æå–æ–‡æœ¬ï¼ˆä¼˜å…ˆæ–‡æœ¬æå–ï¼Œå¿…è¦æ—¶OCRï¼‰
def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    full_text = []
    
    for page_num, page in enumerate(doc):
        # å…ˆå°è¯•æå–æ–‡æœ¬
        if has_selectable_text(page):
            page_text = page.get_text("text")
            if page_text.strip():
                full_text.append(f"[ç¬¬{page_num+1}é¡µ - æ–‡æœ¬æå–]\n{page_text}")
                continue
        
        # å¦‚æœæ–‡æœ¬æå–å¤±è´¥ä¸”Tesseractå¯ç”¨ï¼Œåˆ™ä½¿ç”¨OCR
        if tesseract_available:
            try:
                # å°†é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡
                pix = page.get_pixmap(dpi=300)  # é«˜DPIæé«˜è¯†åˆ«ç²¾åº¦
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                
                # é¢„å¤„ç†å›¾åƒæé«˜è¯†åˆ«ç‡
                img = img.convert('L')  # è½¬ä¸ºç°åº¦å›¾
                img_np = np.array(img)
                # äºŒå€¼åŒ–å¤„ç†ï¼ˆç®€å•é˜ˆå€¼ï¼‰
                threshold = 150
                img_np = (img_np > threshold) * 255
                img = Image.fromarray(img_np.astype(np.uint8))
                
                # è¿›è¡ŒOCRè¯†åˆ«ï¼ˆä¸­è‹±æ–‡ï¼‰
                ocr_text = pytesseract.image_to_string(
                    img, 
                    lang="chi_sim+eng",
                    config='--psm 6'  # å‡è®¾å›¾ç‰‡æ˜¯å•ä¸€å‡åŒ€çš„æ–‡æœ¬å—
                )
                
                if ocr_text.strip():
                    full_text.append(f"[ç¬¬{page_num+1}é¡µ - OCRè¯†åˆ«]\n{ocr_text}")
                else:
                    full_text.append(f"[ç¬¬{page_num+1}é¡µ - æ— æ³•æå–æ–‡æœ¬]")
                    
            except Exception as e:
                full_text.append(f"[ç¬¬{page_num+1}é¡µ - OCRå¤„ç†å¤±è´¥: {str(e)}]")
        else:
            full_text.append(f"[ç¬¬{page_num+1}é¡µ - åŒ…å«å›¾ç‰‡å†…å®¹ï¼Œéœ€è¦Tesseract OCRå¤„ç†]")
    
    doc.close()
    return '\n\n'.join(full_text)

# ç»Ÿä¸€çš„æ–‡ä»¶æå–å‡½æ•°
def extract_text_from_file(uploaded_file, file_type):
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
        if file_type == "docx":
            text = extract_text_from_docx(tmp_path)
        elif file_type == "pdf":
            text = extract_text_from_pdf(tmp_path)
        else:
            text = ""
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_path)
        return text
    
    except Exception as e:
        st.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}")
        return ""

# ä¼˜åŒ–çš„ä¸­æ–‡æ¡æ¬¾æ‹†åˆ†å‡½æ•°
def split_chinese_terms(text):
    # ä¸­æ–‡æ¡æ¬¾å¸¸è§çš„ç¼–å·æ¨¡å¼
    patterns = [
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s?)',  # ç¬¬ä¸€æ¡ ç¬¬äºŒæ¬¾
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s?)',       # ä¸€ã€ äºŒã€
        r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s?)',     # (ä¸€) (äºŒ)
        r'(\d+\.\s?)',                             # 1. 2.1.
        r'(\(\d+\)\s?)',                           # (1) (2)
        r'([A-Za-z]\.\s?)'                         # A. B.
    ]
    
    # ç»„åˆæ‰€æœ‰æ¨¡å¼
    combined_pattern = '|'.join(patterns)
    
    # æ‹†åˆ†æ–‡æœ¬
    parts = re.split(combined_pattern, text)
    
    # é‡ç»„æ¡æ¬¾
    terms = []
    current_term = ""
    
    for part in parts:
        if not part.strip():
            continue
            
        # æ£€æŸ¥å½“å‰éƒ¨åˆ†æ˜¯å¦æ˜¯æ¡æ¬¾ç¼–å·
        is_numbering = any(re.fullmatch(pattern.strip(), part.strip()) for pattern in patterns)
        
        if is_numbering:
            if current_term:  # å¦‚æœå·²æœ‰å†…å®¹ï¼Œä¿å­˜å½“å‰æ¡æ¬¾
                terms.append(current_term.strip())
            current_term = part  # å¼€å§‹æ–°æ¡æ¬¾
        else:
            current_term += part  # æ·»åŠ åˆ°å½“å‰æ¡æ¬¾
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ¡æ¬¾
    if current_term.strip():
        terms.append(current_term.strip())
    
    return terms

# ä½¿ç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œæ¡æ¬¾åŒ¹é…å’Œåˆè§„æ€§åˆ†æ
def analyze_terms_with_qwen(benchmark_term, compare_terms):
    if not qwen_api_key:
        st.error("è¯·å…ˆé…ç½®Qwen APIå¯†é’¥")
        return []
    
    compare_text = "\n".join([f"æ¡æ¬¾{i+1}: {term}" for i, term in enumerate(compare_terms)])
    
    prompt = f"""ä½ æ˜¯ä¸€åæ¡æ¬¾åˆè§„æ€§åˆ†æä¸“å®¶ï¼Œè¯·å¯¹æ¯”ä»¥ä¸‹åŸºå‡†æ¡æ¬¾ä¸å¾…æ¯”è¾ƒæ¡æ¬¾åˆ—è¡¨ï¼Œæ‰¾å‡ºæœ€åŒ¹é…çš„æ¡æ¬¾ã€‚
    åŸºå‡†æ¡æ¬¾: {benchmark_term}
    
    å¾…æ¯”è¾ƒæ¡æ¬¾åˆ—è¡¨:
    {compare_text}
    
    åˆ†æè¦æ±‚:
    1. ä»å¾…æ¯”è¾ƒæ¡æ¬¾ä¸­æ‰¾å‡ºä¸åŸºå‡†æ¡æ¬¾å†…å®¹æœ€ç›¸ä¼¼çš„æ¡æ¬¾
    2. è¯„ä¼°åŒ¹é…åº¦ï¼ˆ0-100åˆ†ï¼‰
    3. ç®€è¦è¯´æ˜åŒ¹é…ç‚¹å’Œå·®å¼‚ç‚¹
    4. å¦‚æœåŒ¹é…åº¦ä½äº70åˆ†ï¼Œåˆ¤å®šä¸ºä¸åˆè§„
    
    è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«:
    - best_match_index: æœ€åŒ¹é…æ¡æ¬¾çš„ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼Œæ²¡æœ‰åˆ™ä¸º-1ï¼‰
    - similarity_score: åŒ¹é…åº¦åˆ†æ•°
    - analysis: åˆ†æè¯´æ˜
    - is_compliant: æ˜¯å¦åˆè§„ï¼ˆtrue/falseï¼‰
    """
    
    try:
        response = Generation.call(
            model="qwen-plus",
            prompt=prompt,
            result_format="json"
        )
        
        if response.status_code == 200:
            result = json.loads(response.output.text)
            return result
        else:
            st.error(f"Qwen APIè°ƒç”¨å¤±è´¥: {response.message}")
            return {"best_match_index": -1, "similarity_score": 0, "analysis": "åˆ†æå¤±è´¥", "is_compliant": False}
    except Exception as e:
        st.error(f"æ¡æ¬¾åˆ†æå‡ºé”™: {str(e)}")
        return {"best_match_index": -1, "similarity_score": 0, "analysis": f"åˆ†æå‡ºé”™: {str(e)}", "is_compliant": False}

# ç”ŸæˆWordæŠ¥å‘Š
def generate_word_report(benchmark_name, compare_results):
    doc = docx.Document()
    
    # æ ‡é¢˜
    title = doc.add_heading("æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # åŸºæœ¬ä¿¡æ¯
    doc.add_paragraph(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    doc.add_paragraph(f"åŸºå‡†æ–‡ä»¶: {benchmark_name}")
    doc.add_paragraph(f"å¯¹æ¯”æ–‡ä»¶æ•°é‡: {len(compare_results)}")
    doc.add_page_break()
    
    # ç›®å½•
    doc.add_heading("ç›®å½•", 1)
    for i, (file_name, _) in enumerate(compare_results.items(), 1):
        p = doc.add_paragraph(f"{i}. {file_name}", style='List Number')
        p.paragraph_format.left_indent = Pt(20)
    
    doc.add_page_break()
    
    # å„æ–‡ä»¶åˆ†æç»“æœ
    for file_name, result in compare_results.items():
        doc.add_heading(file_name, 1)
        
        # åˆè§„æ€§æ¦‚è¦
        compliant_count = sum(1 for item in result if item["analysis"]["is_compliant"])
        total_count = len(result)
        doc.add_heading("åˆè§„æ€§æ¦‚è¦", 2)
        doc.add_paragraph(f"æ€»æ¡æ¬¾æ•°: {total_count}")
        doc.add_paragraph(f"åˆè§„æ¡æ¬¾æ•°: {compliant_count}")
        doc.add_paragraph(f"ä¸åˆè§„æ¡æ¬¾æ•°: {total_count - compliant_count}")
        doc.add_paragraph(f"åˆè§„ç‡: {compliant_count/total_count*100:.2f}%")
        
        # è¯¦ç»†åŒ¹é…ç»“æœ
        doc.add_heading("æ¡æ¬¾åŒ¹é…è¯¦æƒ…", 2)
        for i, item in enumerate(result, 1):
            doc.add_heading(f"åŸºå‡†æ¡æ¬¾ {i}: {item['benchmark_term'][:50]}...", 3)
            
            p = doc.add_paragraph("åŒ¹é…ç»“æœ: ")
            p.add_run(f"{'åˆè§„' if item['analysis']['is_compliant'] else 'ä¸åˆè§„'} ").bold = True
            p.add_run(f"(åŒ¹é…åº¦: {item['analysis']['similarity_score']}åˆ†)")
            
            if item['analysis']['best_match_index'] != -1:
                match_term = item['compare_terms'][item['analysis']['best_match_index']]
                doc.add_paragraph(f"æœ€åŒ¹é…æ¡æ¬¾: {match_term[:100]}...")
            
            doc.add_paragraph("åˆ†æè¯´æ˜:")
            doc.add_paragraph(item['analysis']['analysis'], style='List Bullet')
        
        doc.add_page_break()
    
    # ä¿å­˜åˆ° BytesIO
    buffer = io.BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    
    return buffer

# ä¸»å‡½æ•°
def main():
    # æ–‡ä»¶ä¸Šä¼ 
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("åŸºå‡†æ–‡ä»¶")
        benchmark_file = st.file_uploader("ä¸Šä¼ åŸºå‡†æ–‡ä»¶ (PDFæˆ–DOCX)", type=["pdf", "docx"], key="benchmark")
    
    with col2:
        st.subheader("å¯¹æ¯”æ–‡ä»¶")
        compare_files = st.file_uploader(
            "ä¸Šä¼ å¯¹æ¯”æ–‡ä»¶ (PDFæˆ–DOCXï¼Œå¯å¤šé€‰)", 
            type=["pdf", "docx"], 
            key="compare",
            accept_multiple_files=True
        )
    
    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆ†æ", disabled=not (benchmark_file and compare_files)):
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶å’Œåˆ†ææ¡æ¬¾..."):
            # å¤„ç†åŸºå‡†æ–‡ä»¶
            bench_type = benchmark_file.name.split(".")[-1].lower()
            st.info(f"æ­£åœ¨å¤„ç†åŸºå‡†æ–‡ä»¶: {benchmark_file.name}")
            bench_text = extract_text_from_file(benchmark_file, bench_type)
            
            # æ‹†åˆ†åŸºå‡†æ¡æ¬¾
            st.info("æ­£åœ¨æ‹†åˆ†åŸºå‡†æ¡æ¬¾...")
            bench_terms = split_chinese_terms(bench_text)
            st.success(f"æˆåŠŸæ‹†åˆ†å‡º {len(bench_terms)} æ¡åŸºå‡†æ¡æ¬¾")
            
            # å¤„ç†å¯¹æ¯”æ–‡ä»¶
            compare_results = {}
            
            for compare_file in compare_files:
                file_name = compare_file.name
                st.info(f"æ­£åœ¨å¤„ç†å¯¹æ¯”æ–‡ä»¶: {file_name}")
                
                # æå–æ–‡æœ¬
                compare_type = file_name.split(".")[-1].lower()
                compare_text = extract_text_from_file(compare_file, compare_type)
                
                # æ‹†åˆ†æ¡æ¬¾
                compare_terms = split_chinese_terms(compare_text)
                st.success(f"æˆåŠŸæ‹†åˆ†å‡º {len(compare_terms)} æ¡å¯¹æ¯”æ¡æ¬¾")
                
                # æ¡æ¬¾åŒ¹é…åˆ†æ
                file_results = []
                progress_bar = st.progress(0)
                
                for i, bench_term in enumerate(bench_terms):
                    analysis = analyze_terms_with_qwen(bench_term, compare_terms)
                    file_results.append({
                        "benchmark_term": bench_term,
                        "compare_terms": compare_terms,
                        "analysis": analysis
                    })
                    progress_bar.progress((i + 1) / len(bench_terms))
                
                compare_results[file_name] = file_results
                progress_bar.empty()
            
            # æ˜¾ç¤ºç»“æœ
            st.success("åˆ†æå®Œæˆï¼")
            
            # åˆ›å»ºç»“æœæ ‡ç­¾é¡µ
            tabs = st.tabs(["æ±‡æ€»æŠ¥å‘Š"] + list(compare_results.keys()))
            
            # æ±‡æ€»æŠ¥å‘Š
            with tabs[0]:
                st.subheader("åˆè§„æ€§æ±‡æ€»")
                for i, (file_name, result) in enumerate(compare_results.items(), 1):
                    compliant_count = sum(1 for item in result if item["analysis"]["is_compliant"])
                    total_count = len(result)
                    st.write(f"**{file_name}**")
                    st.write(f"åˆè§„ç‡: {compliant_count/total_count*100:.2f}% ({compliant_count}/{total_count})")
                    
                    non_compliant = [item for item in result if not item["analysis"]["is_compliant"]]
                    if non_compliant:
                        with st.expander(f"æŸ¥çœ‹ä¸åˆè§„æ¡æ¬¾ ({len(non_compliant)})"):
                            for item in non_compliant:
                                st.write(f"**åŸºå‡†æ¡æ¬¾:** {item['benchmark_term'][:100]}...")
                                st.write(f"**åˆ†æ:** {item['analysis']['analysis']}")
                                st.write("---")
            
            # å„æ–‡ä»¶è¯¦ç»†ç»“æœ
            for i, (file_name, result) in enumerate(compare_results.items(), 1):
                with tabs[i]:
                    st.subheader(f"{file_name} åˆ†æç»“æœ")
                    
                    # åˆè§„æ€§æ¦‚è§ˆ
                    compliant_count = sum(1 for item in result if item["analysis"]["is_compliant"])
                    total_count = len(result)
                    st.metric("åˆè§„ç‡", f"{compliant_count/total_count*100:.2f}%", f"{compliant_count}/{total_count}")
                    
                    # è¯¦ç»†æ¡æ¬¾å¯¹æ¯”
                    for j, item in enumerate(result):
                        with st.expander(f"åŸºå‡†æ¡æ¬¾ {j+1}: {item['benchmark_term'][:80]}..."):
                            col_a, col_b = st.columns(2)
                            
                            with col_a:
                                st.write("**åŸºå‡†æ¡æ¬¾å…¨æ–‡:**")
                                st.write(item['benchmark_term'])
                            
                            with col_b:
                                st.write("**åˆ†æç»“æœ:**")
                                st.write(f"åŒ¹é…åº¦: {item['analysis']['similarity_score']}åˆ†")
                                st.write(f"åˆè§„æ€§: {'âœ… åˆè§„' if item['analysis']['is_compliant'] else 'âŒ ä¸åˆè§„'}")
                                
                                if item['analysis']['best_match_index'] != -1:
                                    match_term = item['compare_terms'][item['analysis']['best_match_index']]
                                    st.write("**æœ€åŒ¹é…æ¡æ¬¾:**")
                                    st.write(match_term)
                                
                                st.write("**åˆ†æè¯´æ˜:**")
                                st.write(item['analysis']['analysis'])
            
            # ç”Ÿæˆå¹¶æä¾›ä¸‹è½½æŠ¥å‘Š
            st.subheader("ç”ŸæˆæŠ¥å‘Š")
            report_buffer = generate_word_report(benchmark_file.name, compare_results)
            st.download_button(
                label="ä¸‹è½½åˆè§„æ€§æŠ¥å‘Š (Word)",
                data=report_buffer,
                file_name=f"æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )

if __name__ == "__main__":
    main()
    
