import streamlit as st
import docx
from docx.shared import Pt, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
import re
import os
import tempfile
from datetime import datetime
import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import numpy as np
import requests
import json
from io import BytesIO

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = {}
if 'bench_terms' not in st.session_state:
    st.session_state.bench_terms = []
if 'comparison_terms' not in st.session_state:
    st.session_state.comparison_terms = {}


### 1. å·¥å…·å‡½æ•°ï¼šæ–‡ä»¶è§£æä¸æ–‡æœ¬æå–
def check_tesseract_installation():
    """æ£€æŸ¥Tesseractæ˜¯å¦å®‰è£…"""
    try:
        pytesseract.get_tesseract_version()
        return True
    except:
        return False

def has_selectable_text(page):
    """åˆ¤æ–­PDFé¡µé¢æ˜¯å¦ä¸ºå¯é€‰æ‹©æ–‡æœ¬ï¼ˆéå›¾ç‰‡ï¼‰"""
    text = page.get_text("text").strip()
    # æ–‡æœ¬é•¿åº¦å¤§äº50å­—ç¬¦è®¤ä¸ºæ˜¯å¯é€‰æ‹©æ–‡æœ¬
    return len(text) > 50

def ocr_image(image):
    """å¯¹å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«ï¼ˆä¸­æ–‡ä¼˜å…ˆï¼‰"""
    try:
        # å›¾åƒé¢„å¤„ç†ï¼šè½¬ä¸ºç°åº¦å›¾å¹¶äºŒå€¼åŒ–
        gray_image = image.convert('L')
        threshold = 150
        binary_image = gray_image.point(lambda p: p > threshold and 255)
        
        # æ‰§è¡ŒOCRï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
        text = pytesseract.image_to_string(
            binary_image,
            lang='chi_sim+eng',
            config='--psm 6'  # å‡è®¾å•ä¸€å‡åŒ€æ–‡æœ¬å—
        )
        return text.strip()
    except Exception as e:
        st.warning(f"OCRè¯†åˆ«å‡ºé”™: {str(e)}")
        return ""

def extract_text_from_pdf(pdf_path):
    """ä»PDFæå–æ–‡æœ¬ï¼ˆä¼˜å…ˆæ–‡æœ¬æå–ï¼Œå¿…è¦æ—¶OCRï¼‰"""
    text = []
    try:
        doc = fitz.open(pdf_path)
        tesseract_available = check_tesseract_installation()
        
        for page_num, page in enumerate(doc):
            # ä¼˜å…ˆå°è¯•æ–‡æœ¬æå–
            if has_selectable_text(page):
                page_text = page.get_text("text").strip()
                text.append(f"[é¡µé¢{page_num+1} æ–‡æœ¬æå–]\n{page_text}")
            else:
                # æ–‡æœ¬æå–å¤±è´¥ä¸”Tesseractå¯ç”¨æ—¶ä½¿ç”¨OCR
                if tesseract_available:
                    pix = page.get_pixmap()
                    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                    ocr_result = ocr_image(img)
                    text.append(f"[é¡µé¢{page_num+1} OCRè¯†åˆ«]\n{ocr_result}")
                else:
                    text.append(f"[é¡µé¢{page_num+1} è­¦å‘Šï¼šæ— æ³•æå–æ–‡æœ¬ï¼ˆæœªå®‰è£…Tesseractï¼‰]")
        
        doc.close()
        return "\n\n".join(text)
    except Exception as e:
        st.error(f"PDFè§£æå¤±è´¥: {str(e)}")
        return ""

def extract_text_from_docx(docx_path):
    """ä»DOCXæå–æ–‡æœ¬"""
    try:
        doc = docx.Document(docx_path)
        full_text = []
        for para in doc.paragraphs:
            if para.text.strip():
                full_text.append(para.text.strip())
        return "\n".join(full_text)
    except Exception as e:
        st.error(f"DOCXè§£æå¤±è´¥: {str(e)}")
        return ""

def extract_text_from_file(uploaded_file, file_type):
    """ç»Ÿä¸€æ–‡ä»¶æå–å…¥å£"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_path = temp_file.name
        
        if file_type == "pdf":
            return extract_text_from_pdf(temp_path)
        elif file_type == "docx":
            return extract_text_from_docx(temp_path)
        else:
            return ""
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.unlink(temp_path)


### 2. ä¸­æ–‡æ¡æ¬¾æ‹†åˆ†å‡½æ•°ï¼ˆä¿®å¤æ­£åˆ™é”™è¯¯ç‰ˆï¼‰
def split_chinese_terms(text, max_terms=50):
    """æ‹†åˆ†ä¸­æ–‡æ¡æ¬¾ï¼ˆä¿®å¤æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯ï¼Œæ”¯æŒæœ€å¤§æ¡æ¬¾æ•°é™åˆ¶ï¼‰"""
    # è¾“å…¥éªŒè¯
    if not text or not isinstance(text, str):
        st.warning("è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–æ— æ•ˆï¼Œæ— æ³•æ‹†åˆ†æ¡æ¬¾")
        return []
    
    # é¢„å¤„ç†ï¼šæ¸…é™¤å¤šä½™ç©ºè¡Œå’Œç©ºæ ¼ï¼Œç»Ÿä¸€æ ‡ç‚¹
    processed_text = re.sub(r'\n+', '\n', text.strip())
    processed_text = re.sub(r'\s+', ' ', processed_text)
    # æ›¿æ¢å…¨è§’æ ‡ç‚¹ä¸ºåŠè§’ï¼Œä¾¿äºç»Ÿä¸€å¤„ç†
    processed_text = processed_text.replace('ã€‚', '.').replace('ï¼Œ', ',').replace('ï¼›', ';')
    processed_text = processed_text.replace('ï¼š', ':').replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    
    # ä¸­æ–‡æ¡æ¬¾å¸¸è§ç¼–å·æ ¼å¼ï¼ˆæ­£åˆ™æ¨¡å¼ï¼‰
    patterns = [
        r'(\d+\.\d+\.\d+\s+)',        # 1.1.1 
        r'(\d+\.\d+\s+)',             # 1.1 
        r'(\d+\.\s+)',                # 1. 
        r'(\(\d+\)\.\s+)',            # (1). 
        r'(\(\d+\)\s+)',              # (1) 
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+)',  # ä¸€ã€ 
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¡\s+)', # ç¬¬ä¸€æ¡
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+æ¬¾\s+)', # ç¬¬ä¸€æ¬¾
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+é¡¹\s+)', # ç¬¬ä¸€é¡¹
        r'(\d+\)\s+)',                # 1)
        r'([A-Za-z]\.\s+)',           # A. 
        r'([A-Za-z]\)\s+)',           # A)
    ]
    
    try:
        # ç»„åˆæ‰€æœ‰æ¨¡å¼
        combined_pattern = '|'.join(patterns)
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ¡æ¬¾ç¼–å·ä½ç½®
        matches = list(re.finditer(combined_pattern, processed_text, re.MULTILINE))
        
        if not matches:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†ç¼–å·ï¼Œå°è¯•æŒ‰ç©ºè¡Œæ‹†åˆ†
            st.info("æœªæ£€æµ‹åˆ°æ ‡å‡†æ¡æ¬¾ç¼–å·ï¼Œå°è¯•æŒ‰ç©ºè¡Œæ‹†åˆ†")
            terms = [t.strip() for t in re.split(r'\n\s*\n', processed_text) if t.strip()]
            # åº”ç”¨æœ€å¤§æ¡æ¬¾æ•°é™åˆ¶
            return terms[:max_terms] if max_terms > 0 else []
        
        # æå–æ¡æ¬¾å†…å®¹
        terms = []
        # ç¬¬ä¸€æ¡æ¡æ¬¾ä»æ–‡æœ¬å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…
        first_match = matches[0]
        if first_match.start() > 0:
            pre_text = processed_text[:first_match.start()].strip()
            if pre_text:
                terms.append(pre_text)
        
        # å¤„ç†ä¸­é—´çš„æ¡æ¬¾
        for i in range(len(matches)):
            current_match = matches[i]
            start_idx = current_match.start()
            
            # ç¡®å®šå½“å‰æ¡æ¬¾çš„ç»“æŸä½ç½®
            if i < len(matches) - 1:
                end_idx = matches[i+1].start()
            else:
                end_idx = len(processed_text)
            
            # æå–æ¡æ¬¾å†…å®¹ï¼ˆåŒ…å«ç¼–å·ï¼‰
            term_content = processed_text[start_idx:end_idx].strip()
            if term_content:
                terms.append(term_content)
            
            # è¾¾åˆ°æœ€å¤§æ¡æ¬¾æ•°åˆ™åœæ­¢
            if max_terms > 0 and len(terms) >= max_terms:
                break
        
        # è¿‡æ»¤è¿‡çŸ­çš„æ¡æ¬¾ï¼ˆå¯èƒ½æ˜¯è¯¯æ‹†åˆ†ï¼‰
        min_term_length = 10  # æœ€å°æ¡æ¬¾é•¿åº¦
        terms = [term for term in terms if len(term) >= min_term_length]
        
        # åº”ç”¨æœ€å¤§æ¡æ¬¾æ•°é™åˆ¶
        limited_terms = terms[:max_terms] if max_terms > 0 else []
        
        # æ‹†åˆ†æ•ˆæœåé¦ˆ
        st.success(f"æˆåŠŸæ‹†åˆ†æ¡æ¬¾ï¼š{len(limited_terms)}æ¡ï¼ˆé™åˆ¶æœ€å¤§{max_terms}æ¡ï¼‰")
        return limited_terms
        
    except re.error as e:
        st.error(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {str(e)}")
        # å‡ºé”™æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼šæŒ‰ç©ºè¡Œæ‹†åˆ†
        st.info("ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆæ‹†åˆ†æ¡æ¬¾")
        terms = [t.strip() for t in re.split(r'\n\s*\n', processed_text) if t.strip()]
        return terms[:max_terms] if max_terms > 0 else []
    except Exception as e:
        st.error(f"æ¡æ¬¾æ‹†åˆ†å¤±è´¥: {str(e)}")
        return [processed_text[:500]] if max_terms > 0 else []  # è¿”å›éƒ¨åˆ†æ–‡æœ¬ä½œä¸ºå¤‡é€‰


### 3. Qwenå¤§æ¨¡å‹è°ƒç”¨ï¼ˆå…¼å®¹æ¨¡å¼APIï¼‰
def call_qwen_api(prompt, api_key):
    """è°ƒç”¨é˜¿é‡Œäº‘DashScopeå…¼å®¹æ¨¡å¼API"""
    if not api_key:
        return None, "æœªæä¾›APIå¯†é’¥"
    
    url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": "qwen-plus",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3  # ä½æ¸©åº¦ï¼Œä¿è¯ç»“æœç¨³å®š
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()
        
        if "choices" in result and len(result["choices"]) > 0:
            return result["choices"][0]["message"]["content"], None
        else:
            return None, f"APIè¿”å›æ ¼å¼å¼‚å¸¸: {str(result)}"
    except Exception as e:
        return None, f"APIè°ƒç”¨å¤±è´¥: {str(e)}"

def analyze_terms_with_qwen(bench_term, compare_term, api_key):
    """ç”¨Qwenåˆ†ææ¡æ¬¾åŒ¹é…åº¦"""
    prompt = f"""è¯·å¯¹æ¯”ä»¥ä¸‹ä¸¤ä¸ªæ¡æ¬¾çš„åŒ¹é…åº¦ï¼š
    ã€åŸºå‡†æ¡æ¬¾ã€‘ï¼š{bench_term[:500]}
    ã€å¾…æ¯”æ¡æ¬¾ã€‘ï¼š{compare_term[:500]}
    
    è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
    1. åŒ¹é…åº¦ï¼ˆ0-100åˆ†ï¼‰ï¼š[åˆ†æ•°]
    2. ç›¸åŒç‚¹ï¼š[ç®€è¦è¯´æ˜ç›¸åŒå†…å®¹]
    3. åŒ¹é…ä¾æ®ï¼š[è¯´æ˜ä¸ºä»€ä¹ˆè®¤ä¸ºè¿™ä¸¤ä¸ªæ¡æ¬¾åŒ¹é…]
    """
    
    result, error = call_qwen_api(prompt, api_key)
    if error:
        return None, error
    
    # è§£æç»“æœ
    try:
        score_match = re.search(r'åŒ¹é…åº¦ï¼ˆ0-100åˆ†ï¼‰ï¼š(\d+)', result)
        score = int(score_match.group(1)) if score_match else 0
        
        return {
            "score": score,
            "full_analysis": result
        }, None
    except:
        return {
            "score": 0,
            "full_analysis": f"è§£æå¤±è´¥ï¼ŒåŸå§‹ç»“æœï¼š{result}"
        }, None


### 4. ç»“æœæŠ¥å‘Šç”Ÿæˆ
def generate_word_report(bench_terms, comparison_results, bench_filename, max_terms):
    """ç”ŸæˆåªåŒ…å«åŒ¹é…æ¡æ¬¾çš„WordæŠ¥å‘Š"""
    doc = docx.Document()
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    style = doc.styles['Normal']
    style.font.name = 'SimSun'
    style._element.rPr.rFonts.set(qn('w:eastAsia'), 'SimSun')
    style.font.size = Pt(10.5)
    
    # æ ‡é¢˜
    title = doc.add_heading("æ¡æ¬¾åŒ¹é…åˆ†ææŠ¥å‘Š", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # åŸºæœ¬ä¿¡æ¯
    doc.add_paragraph(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}")
    doc.add_paragraph(f"åŸºå‡†æ–‡ä»¶ï¼š{bench_filename}")
    doc.add_paragraph(f"å¯¹æ¯”æ–‡ä»¶æ•°é‡ï¼š{len(comparison_results)}")
    doc.add_paragraph(f"æœ€å¤§è§£ææ¡æ¬¾æ•°ï¼š{max_terms}")
    doc.add_page_break()
    
    # æŒ‰æ–‡ä»¶ç”Ÿæˆç»“æœ
    for file_name, matched_terms in comparison_results.items():
        doc.add_heading(f"å¯¹æ¯”æ–‡ä»¶ï¼š{file_name}", level=1)
        
        # å¯åŒ¹é…æ¡æ¬¾
        doc.add_heading(f"åŒ¹é…æ¡æ¬¾ï¼ˆå…±{len(matched_terms)}æ¡ï¼‰", level=2)
        if matched_terms:
            for idx, item in enumerate(matched_terms, 1):
                doc.add_heading(f"{idx}. åŸºå‡†æ¡æ¬¾ï¼š{item['bench_term'][:30]}...", level=3)
                doc.add_paragraph(f"å¯¹æ¯”æ¡æ¬¾ï¼š{item['compare_term'][:50]}...")
                doc.add_paragraph(f"åŒ¹é…åº¦ï¼š{item['analysis']['score']}åˆ†")
                doc.add_paragraph("åŒ¹é…åˆ†æï¼š")
                doc.add_paragraph(item['analysis']['full_analysis'], style='Normal')
        else:
            doc.add_paragraph("æœªå‘ç°åŒ¹é…æ¡æ¬¾")
        
        doc.add_page_break()
    
    # ä¿å­˜åˆ°å†…å­˜
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer


### 5. ä¸»å‡½æ•°
def main():
    st.title("ğŸ“„ æ¡æ¬¾åŒ¹é…åˆ†æå·¥å…·")
    st.write("æ”¯æŒä¸Šä¼ åŸºå‡†æ–‡ä»¶å’Œå¤šä¸ªå¯¹æ¯”æ–‡ä»¶ï¼Œåªå±•ç¤ºåŒ¹é…çš„æ¡æ¬¾å¹¶ç”ŸæˆæŠ¥å‘Š")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.subheader("é…ç½®")
        qwen_api_key = st.text_input("é˜¿é‡Œäº‘DashScope APIå¯†é’¥", type="password")
        st.info("è·å–å¯†é’¥ï¼šhttps://dashscope.console.aliyun.com/")
        
        # æ·»åŠ æœ€å¤§è§£ææ¡ä¾‹æ•°è®¾ç½®
        max_terms = st.slider(
            "æœ€å¤§è§£ææ¡æ¬¾æ•°",
            min_value=0,
            max_value=50,
            value=20,
            help="è®¾ç½®0-50ä¹‹é—´çš„æ•°å€¼ï¼Œé™åˆ¶è§£æçš„æœ€å¤§æ¡æ¬¾æ•°é‡"
        )
        
        # åŒ¹é…åº¦é˜ˆå€¼è®¾ç½®
        match_threshold = st.slider(
            "åŒ¹é…åº¦é˜ˆå€¼ï¼ˆåˆ†ï¼‰",
            min_value=0,
            max_value=100,
            value=70,
            help="é«˜äºæ­¤åˆ†æ•°çš„æ¡æ¬¾å°†è¢«è§†ä¸ºåŒ¹é…"
        )
        
        st.divider()
        st.subheader("ä½¿ç”¨è¯´æ˜")
        st.write("1. ä¸Šä¼ 1ä¸ªåŸºå‡†æ–‡ä»¶å’Œå¤šä¸ªå¯¹æ¯”æ–‡ä»¶")
        st.write("2. é…ç½®æœ€å¤§æ¡æ¬¾æ•°å’ŒåŒ¹é…åº¦é˜ˆå€¼")
        st.write("3. ç‚¹å‡»å¼€å§‹åˆ†æ")
        st.write("4. æŸ¥çœ‹åŒ¹é…æ¡æ¬¾å¹¶ä¸‹è½½æŠ¥å‘Š")
    
    # æ–‡ä»¶ä¸Šä¼ 
    col1, col2 = st.columns(2)
    with col1:
        bench_file = st.file_uploader("ä¸Šä¼ åŸºå‡†æ–‡ä»¶ï¼ˆPDF/DOCXï¼‰", type=["pdf", "docx"], accept_multiple_files=False)
    with col2:
        compare_files = st.file_uploader("ä¸Šä¼ å¯¹æ¯”æ–‡ä»¶ï¼ˆPDF/DOCXï¼‰", type=["pdf", "docx"], accept_multiple_files=True)
    
    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆ†æ", disabled=not (bench_file and compare_files and max_terms > 0)):
        with st.spinner("æ­£åœ¨å¤„ç†åŸºå‡†æ–‡ä»¶..."):
            # 1. æå–åŸºå‡†æ–‡ä»¶æ–‡æœ¬å¹¶æ‹†åˆ†æ¡æ¬¾ï¼ˆåº”ç”¨æœ€å¤§æ•°é‡é™åˆ¶ï¼‰
            bench_type = bench_file.name.split('.')[-1].lower()
            bench_text = extract_text_from_file(bench_file, bench_type)
            
            # æ˜¾ç¤ºæå–çš„æ–‡æœ¬é¢„è§ˆ
            with st.expander("æŸ¥çœ‹åŸºå‡†æ–‡ä»¶æå–æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰"):
                st.text(bench_text[:500])
            
            bench_terms = split_chinese_terms(bench_text, max_terms)
            st.session_state.bench_terms = bench_terms
            st.success(f"åŸºå‡†æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæå–æ¡æ¬¾ï¼š{len(bench_terms)}æ¡")
        
        # 2. å¤„ç†æ¯ä¸ªå¯¹æ¯”æ–‡ä»¶
        all_results = {}
        progress_bar = st.progress(0)
        
        for file_idx, compare_file in enumerate(compare_files):
            file_name = compare_file.name
            st.subheader(f"å¤„ç†å¯¹æ¯”æ–‡ä»¶ï¼š{file_name}")
            
            # æå–æ–‡æœ¬å¹¶æ‹†åˆ†æ¡æ¬¾ï¼ˆåº”ç”¨æœ€å¤§æ•°é‡é™åˆ¶ï¼‰
            compare_type = file_name.split('.')[-1].lower()
            compare_text = extract_text_from_file(compare_file, compare_type)
            
            # æ˜¾ç¤ºæå–çš„æ–‡æœ¬é¢„è§ˆ
            with st.expander(f"æŸ¥çœ‹{file_name}æå–æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰"):
                st.text(compare_text[:500])
            
            compare_terms = split_chinese_terms(compare_text, max_terms)
            st.session_state.comparison_terms[file_name] = compare_terms
            st.info(f"æå–æ¡æ¬¾ï¼š{len(compare_terms)}æ¡")
            
            # æ¡æ¬¾åŒ¹é…åˆ†æ
            matched_terms = []
            
            with st.spinner(f"æ­£åœ¨åˆ†æ {file_name} çš„æ¡æ¬¾åŒ¹é…åº¦..."):
                # ä¸ºæ¯ä¸ªåŸºå‡†æ¡æ¬¾å¯»æ‰¾æœ€ä½³åŒ¹é…
                for bench_term in bench_terms:
                    best_match = None
                    highest_score = 0
                    
                    # ä¸æ‰€æœ‰å¯¹æ¯”æ¡æ¬¾æ¯”è¾ƒ
                    for compare_term in compare_terms:
                        # è°ƒç”¨Qwenåˆ†æï¼ˆæ— APIå¯†é’¥åˆ™è·³è¿‡ï¼‰
                        if qwen_api_key:
                            analysis, error = analyze_terms_with_qwen(bench_term, compare_term, qwen_api_key)
                            if error:
                                st.warning(f"æ¡æ¬¾åˆ†æå¤±è´¥ï¼š{error}")
                                continue
                        else:
                            # æ— APIæ—¶çš„åŸºç¡€åˆ¤æ–­
                            common_words = len(set(bench_term[:100].split()) & set(compare_term[:100].split()))
                            score = min(100, common_words * 5)  # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
                            analysis = {
                                "score": score,
                                "full_analysis": "æœªä½¿ç”¨Qwen APIï¼ŒåŸºäºå…³é”®è¯åŒ¹é…"
                            }
                    
                        # è·Ÿè¸ªæœ€é«˜åˆ†åŒ¹é…
                        if analysis["score"] > highest_score:
                            highest_score = analysis["score"]
                            best_match = {
                                "bench_term": bench_term,
                                "compare_term": compare_term,
                                "analysis": analysis
                            }
                    
                    # å¦‚æœæ‰¾åˆ°é«˜äºé˜ˆå€¼çš„åŒ¹é…é¡¹ï¼Œåˆ™æ·»åŠ 
                    if best_match and highest_score >= match_threshold:
                        matched_terms.append(best_match)
            
            # ä¿å­˜ç»“æœ
            all_results[file_name] = matched_terms
            st.success(f"{file_name} åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(matched_terms)} æ¡åŒ¹é…æ¡æ¬¾")
            
            # æ›´æ–°è¿›åº¦
            progress_bar.progress((file_idx + 1) / len(compare_files))
        
        # 3. å±•ç¤ºç»“æœ
        st.session_state.analysis_results = all_results
        st.success("æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæˆï¼")
        
        # æ˜¾ç¤ºåŒ¹é…ç»“æœ
        for file_name, matched_terms in all_results.items():
            st.subheader(f"{file_name} çš„åŒ¹é…æ¡æ¬¾ï¼ˆ{len(matched_terms)}æ¡ï¼‰")
            for idx, item in enumerate(matched_terms, 1):
                with st.expander(f"åŒ¹é…é¡¹ {idx}ï¼ˆåŒ¹é…åº¦ï¼š{item['analysis']['score']}åˆ†ï¼‰"):
                    col_a, col_b = st.columns(2)
                    with col_a:
                        st.write("**åŸºå‡†æ¡æ¬¾ï¼š**")
                        st.write(item['bench_term'])
                    with col_b:
                        st.write("**å¯¹æ¯”æ¡æ¬¾ï¼š**")
                        st.write(item['compare_term'])
                    st.write("**åŒ¹é…åˆ†æï¼š**")
                    st.write(item['analysis']['full_analysis'])
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        if st.button("ç”ŸæˆWordæŠ¥å‘Š"):
            with st.spinner("æ­£åœ¨ç”ŸæˆæŠ¥å‘Š..."):
                report_buffer = generate_word_report(
                    bench_terms, 
                    all_results, 
                    bench_file.name,
                    max_terms
                )
                st.download_button(
                    label="ä¸‹è½½æŠ¥å‘Š",
                    data=report_buffer,
                    file_name=f"æ¡æ¬¾åŒ¹é…åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )


if __name__ == "__main__":
    main()
    
