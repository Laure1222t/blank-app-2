import streamlit as st
import fitz  # PyMuPDF
import re
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
import tempfile
import os

# è®¾ç½®é¡µé¢é…ç½® - ä¼˜åŒ–ç§»åŠ¨ç«¯æ˜¾ç¤º
st.set_page_config(
    page_title="æ”¿ç­–æ–‡ä»¶æ¯”å¯¹åˆ†æå·¥å…·",
    page_icon="ğŸ“œ",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# è‡ªå®šä¹‰CSS - ä¼˜åŒ–æ˜¾ç¤ºæ•ˆæœ
st.markdown("""
<style>
    .stButton>button {
        width: 100%;
        margin-top: 1rem;
    }
    .analysis-box {
        border: 1px solid #e0e0e0;
        border-radius: 5px;
        padding: 1rem;
        margin-top: 1rem;
    }
    .section-header {
        margin-top: 1.5rem;
        margin-bottom: 0.5rem;
        font-weight: bold;
        color: #2c3e50;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ - é¿å…é‡å¤åŠ è½½
if 'target_clauses' not in st.session_state:
    st.session_state.target_clauses = []
if 'compare_clauses' not in st.session_state:
    st.session_state.compare_clauses = []
if 'analysis_result' not in st.session_state:
    st.session_state.analysis_result = None
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'tokenizer' not in st.session_state:
    st.session_state.tokenizer = None
if 'model' not in st.session_state:
    st.session_state.model = None

# é¡µé¢æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ“œ ä¸­æ–‡æ”¿ç­–æ–‡ä»¶æ¯”å¯¹åˆ†æå·¥å…·")
st.markdown("ä¸Šä¼ ç›®æ ‡æ”¿ç­–æ–‡ä»¶å’Œå¾…æ¯”å¯¹æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è§£æå¹¶è¿›è¡Œæ¡æ¬¾æ¯”å¯¹ä¸åˆè§„æ€§åˆ†æ")
st.markdown("---")

# æ£€æŸ¥PyTorchæ˜¯å¦å¯ç”¨
try:
    import torch
    torch_available = True
except ImportError:
    torch_available = False
    st.error("âš ï¸ æœªæ£€æµ‹åˆ°PyTorchåº“ï¼Œè¯·æ£€æŸ¥ä¾èµ–é…ç½®ã€‚")

# ä¼˜åŒ–çš„PDFè§£æå‡½æ•° - æ›´å‡†ç¡®çš„æ¡æ¬¾æå–
def parse_pdf(file):
    """è§£æPDFæ–‡ä»¶å¹¶æå–ç»“æ„åŒ–æ¡æ¬¾"""
    try:
        # è¯»å–PDFå†…å®¹
        with st.spinner("æ­£åœ¨è§£ææ–‡ä»¶..."):
            doc = fitz.open(stream=file.read(), filetype="pdf")
            text = ""
            for page in doc:
                text += page.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            text = re.sub(r'\s+', ' ', text).strip()
            
            # æ¡æ¬¾æå–ç­–ç•¥ï¼šä¼˜å…ˆè¯†åˆ«å¤šçº§ç¼–å·æ¡æ¬¾
            clause_patterns = [
                re.compile(r'(\d+\.\s+.*?)(?=\d+\.\s+|$)', re.DOTALL),  # ä¸€çº§æ¡æ¬¾ (1. ...)
                re.compile(r'(\d+\.\d+\s+.*?)(?=\d+\.\d+\s+|\d+\.\s+|$)', re.DOTALL),  # äºŒçº§æ¡æ¬¾ (1.1 ...)
                re.compile(r'(\d+\.\d+\.\d+\s+.*?)(?=\d+\.\d+\.\d+\s+|\d+\.\d+\s+|$)', re.DOTALL)  # ä¸‰çº§æ¡æ¬¾
            ]
            
            clauses = []
            for pattern in clause_patterns:
                matches = pattern.findall(text)
                if matches:
                    clauses = [match.strip() for match in matches if len(match.strip()) > 20]  # è¿‡æ»¤è¿‡çŸ­æ¡ç›®
                    break
            
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°æ¡æ¬¾æ ¼å¼ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            if not clauses:
                paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 50]  # è¿‡æ»¤è¿‡çŸ­æ®µè½
                clauses = paragraphs
            
            return clauses[:30]  # é™åˆ¶æœ€å¤§æ¡æ¬¾æ•°é‡ï¼Œé¿å…å†…å­˜é—®é¢˜
            
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æé”™è¯¯: {str(e)}")
        return []

# æ¨¡å‹åŠ è½½ä¼˜åŒ– - ä½¿ç”¨é‡åŒ–æŠ€æœ¯å‡å°‘å†…å­˜å ç”¨
@st.cache_resource(show_spinner=False)
def load_optimized_model():
    """åŠ è½½é‡åŒ–åçš„Qwenæ¨¡å‹ï¼Œé€‚åˆäº‘ç¯å¢ƒè¿è¡Œ"""
    if not torch_available:
        return None, None
        
    try:
        # 4ä½é‡åŒ–é…ç½® - å¤§å¹…å‡å°‘å†…å­˜ä½¿ç”¨
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        
        # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ç‰ˆæœ¬ï¼Œé€‚åˆäº‘ç¯å¢ƒ
        model_name = "Qwen/Qwen-1.8B-Chat"
        
        with st.spinner(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name}...\nè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´"):
            tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                trust_remote_code=True,
                cache_dir="./cache"
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                cache_dir="./cache"
            )
            model.eval()
            
            return tokenizer, model
            
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        st.info("æç¤ºï¼šå¦‚æœæŒç»­åŠ è½½å¤±è´¥ï¼Œå¯èƒ½æ˜¯èµ„æºé™åˆ¶ï¼Œè¯·å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æœ¬åœ°éƒ¨ç½²ã€‚")
        return None, None

# åˆè§„æ€§åˆ†æå‡½æ•°ä¼˜åŒ– - æ›´æ˜ç¡®çš„æç¤ºè¯å’Œæ‰¹å¤„ç†
def analyze_compliance(target_clauses, compare_clauses):
    """ä½¿ç”¨ä¼˜åŒ–çš„æç¤ºè¯è¿›è¡Œåˆè§„æ€§åˆ†æ"""
    if not st.session_state.tokenizer or not st.session_state.model:
        return "æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚"
    
    try:
        with st.spinner("æ­£åœ¨è¿›è¡Œæ¡æ¬¾æ¯”å¯¹å’Œåˆè§„æ€§åˆ†æ..."):
            # å‡†å¤‡æ¡æ¬¾æ–‡æœ¬ - é™åˆ¶é•¿åº¦ä»¥é€‚åº”æ¨¡å‹
            target_text = "\n".join([f"æ¡æ¬¾{i+1}: {clause[:200]}" for i, clause in enumerate(target_clauses[:15])])
            compare_text = "\n".join([f"æ¡æ¬¾{i+1}: {clause[:200]}" for i, clause in enumerate(compare_clauses[:15])])
            
            # ä¼˜åŒ–çš„æç¤ºè¯ - æ›´æ˜ç¡®çš„æŒ‡ä»¤
            prompt = """
            ä½ æ˜¯æ”¿ç­–åˆè§„æ€§åˆ†æä¸“å®¶ï¼Œéœ€è¦æ¯”å¯¹ä¸¤ä»½æ–‡ä»¶çš„æ¡æ¬¾å¹¶è¿›è¡Œåˆè§„æ€§åˆ†æã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚æ‰§è¡Œï¼š
            
            1. å…¨é¢è¦†ç›–æä¾›çš„æ‰€æœ‰æ¡æ¬¾ï¼Œä¸è¦é—æ¼é‡è¦å†…å®¹
            2. é‡ç‚¹åˆ†æåˆè§„æ€§ï¼šå¯¹äºä¸åŒä¹‹å¤„ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨å†²çªã€ä¸ä¸€è‡´æˆ–ä¸åˆè§„çš„æƒ…å†µ
            3. å¯¹äºç›¸åŒæˆ–ä¸€è‡´çš„æ¡æ¬¾ï¼Œç®€è¦è¯´æ˜å³å¯
            4. åˆ†ææ—¶è¯·åŸºäºæ¡æ¬¾å†…å®¹æœ¬èº«ï¼Œä¸è¦æ·»åŠ å¤–éƒ¨çŸ¥è¯†
            5. è¾“å‡ºæ ¼å¼ï¼š
               - å…ˆåˆ—å‡ºæ¡æ¬¾å¯¹åº”å…³ç³»
               - å†åˆ†æå·®å¼‚ç‚¹
               - æœ€åç»™å‡ºåˆè§„æ€§åˆ¤æ–­åŠå»ºè®®
            
            ç›®æ ‡æ”¿ç­–æ–‡ä»¶æ¡æ¬¾ï¼š
            {target_text}
            
            å¾…æ¯”å¯¹æ–‡ä»¶æ¡æ¬¾ï¼š
            {compare_text}
            
            è¯·ç”¨ä¸­æ–‡è¯¦ç»†è¾“å‡ºåˆ†æç»“æœï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°ã€ç»“è®ºæ˜ç¡®ã€‚
            """.format(target_text=target_text, compare_text=compare_text)
            
            # æ¨¡å‹æ¨ç†å‚æ•°ä¼˜åŒ–
            inputs = st.session_state.tokenizer(prompt, return_tensors="pt").to(st.session_state.model.device)
            
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
                outputs = st.session_state.model.generate(
                    **inputs,
                    max_new_tokens=1200,  # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œé¿å…è¶…æ—¶
                    temperature=0.6,      # é™ä½éšæœºæ€§ï¼Œæé«˜ç¨³å®šæ€§
                    top_p=0.9,
                    repetition_penalty=1.1  # å‡å°‘é‡å¤å†…å®¹
                )
            
            result = st.session_state.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æœ‰æ•ˆç»“æœï¼ˆå»é™¤æç¤ºè¯éƒ¨åˆ†ï¼‰
            result_start = result.find("ç›®æ ‡æ”¿ç­–æ–‡ä»¶æ¡æ¬¾ï¼š")
            if result_start != -1:
                result = result[result_start:]
                
            return result
            
    except Exception as e:
        st.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        return f"åˆ†æå¤±è´¥: {str(e)}"

# ç”ŸæˆWordæ–‡æ¡£å‡½æ•°ä¼˜åŒ–
def generate_word_document(analysis_result, target_filename, compare_filename):
    """ç”Ÿæˆæ ¼å¼åŒ–çš„Wordåˆ†ææŠ¥å‘Š"""
    try:
        doc = Document()
        
        # æ ‡é¢˜
        title = doc.add_heading("æ”¿ç­–æ–‡ä»¶åˆè§„æ€§åˆ†ææŠ¥å‘Š", 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # åŸºæœ¬ä¿¡æ¯
        doc.add_paragraph(f"ç›®æ ‡æ”¿ç­–æ–‡ä»¶: {target_filename}")
        doc.add_paragraph(f"å¾…æ¯”å¯¹æ–‡ä»¶: {compare_filename}")
        doc.add_paragraph(f"åˆ†ææ—¥æœŸ: {time.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        doc.add_paragraph("")
        
        # åˆ†æç»“æœ
        doc.add_heading("ä¸€ã€åˆ†æç»“æœ", level=1)
        
        # å¤„ç†åˆ†æç»“æœä¸ºæ®µè½
        paragraphs = re.split(r'\n+', analysis_result)
        for para in paragraphs:
            para = para.strip()
            if para:
                # è¯†åˆ«æ ‡é¢˜è¡Œ
                if para.startswith(('1.', '2.', '3.')) or para.endswith('ï¼š'):
                    p = doc.add_paragraph(para)
                    p.style = 'Heading 2'
                else:
                    p = doc.add_paragraph(para)
                    p.paragraph_format.space_after = Pt(6)
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:
            doc.save(tmp.name)
            return tmp.name
            
    except Exception as e:
        st.error(f"ç”ŸæˆWordæ–‡æ¡£å¤±è´¥: {str(e)}")
        return None

# ä¸»ç•Œé¢å¸ƒå±€
col1, col2 = st.columns(2, gap="large")

with col1:
    st.subheader("ç›®æ ‡æ”¿ç­–æ–‡ä»¶")
    st.caption("ä½œä¸ºåŸºå‡†çš„æ”¿ç­–æ–‡ä»¶")
    target_file = st.file_uploader("ä¸Šä¼ ç›®æ ‡æ”¿ç­–æ–‡ä»¶ (PDF)", type="pdf", key="target")
    
    if target_file:
        st.session_state.target_clauses = parse_pdf(target_file)
        st.success(f"âœ… è§£æå®Œæˆï¼Œæå–åˆ° {len(st.session_state.target_clauses)} æ¡æ¡æ¬¾")
        
        with st.expander(f"æŸ¥çœ‹æå–çš„æ¡æ¬¾ (æ˜¾ç¤ºå‰10æ¡)"):
            for i, clause in enumerate(st.session_state.target_clauses[:10]):
                st.markdown(f"**æ¡æ¬¾ {i+1}:** {clause[:150]}..." if len(clause) > 150 else f"**æ¡æ¬¾ {i+1}:** {clause}")

with col2:
    st.subheader("å¾…æ¯”å¯¹æ–‡ä»¶")
    st.caption("éœ€è¦æ£€æŸ¥åˆè§„æ€§çš„æ–‡ä»¶")
    compare_file = st.file_uploader("ä¸Šä¼ å¾…æ¯”å¯¹æ–‡ä»¶ (PDF)", type="pdf", key="compare")
    
    if compare_file:
        st.session_state.compare_clauses = parse_pdf(compare_file)
        st.success(f"âœ… è§£æå®Œæˆï¼Œæå–åˆ° {len(st.session_state.compare_clauses)} æ¡æ¡æ¬¾")
        
        with st.expander(f"æŸ¥çœ‹æå–çš„æ¡æ¬¾ (æ˜¾ç¤ºå‰10æ¡)"):
            for i, clause in enumerate(st.session_state.compare_clauses[:10]):
                st.markdown(f"**æ¡æ¬¾ {i+1}:** {clause[:150]}..." if len(clause) > 150 else f"**æ¡æ¬¾ {i+1}:** {clause}")

# æ¨¡å‹åŠ è½½å’Œåˆ†ææ§åˆ¶
st.markdown("---")

# å•ç‹¬çš„æ¨¡å‹åŠ è½½æŒ‰é’®ï¼Œé¿å…é‡å¤åŠ è½½
if not st.session_state.model_loaded and torch_available:
    if st.button("ğŸ“¦ åŠ è½½åˆ†ææ¨¡å‹ (é¦–æ¬¡ä½¿ç”¨éœ€è¦å‡ åˆ†é’Ÿ)"):
        st.session_state.tokenizer, st.session_state.model = load_optimized_model()
        if st.session_state.tokenizer and st.session_state.model:
            st.session_state.model_loaded = True
            st.success("æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¯ä»¥å¼€å§‹åˆ†æäº†ï¼")
        else:
            st.session_state.model_loaded = False

# åˆ†ææŒ‰é’®
if st.session_state.model_loaded and st.session_state.target_clauses and st.session_state.compare_clauses:
    if st.button("ğŸ” å¼€å§‹æ¯”å¯¹ä¸åˆè§„æ€§åˆ†æ"):
        with st.spinner("æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æï¼Œè¯·ç¨å€™..."):
            st.session_state.analysis_result = analyze_compliance(
                st.session_state.target_clauses, 
                st.session_state.compare_clauses
            )

# æ˜¾ç¤ºåˆ†æç»“æœ
if st.session_state.analysis_result:
    st.markdown("### ğŸ“Š åˆè§„æ€§åˆ†æç»“æœ")
    st.markdown('<div class="analysis-box">', unsafe_allow_html=True)
    # å°†åˆ†æç»“æœæŒ‰æ®µè½æ˜¾ç¤ºï¼Œå¢å¼ºå¯è¯»æ€§
    for para in re.split(r'\n+', st.session_state.analysis_result):
        if para.strip():
            st.markdown(f"{para.strip()}  \n")
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ç”Ÿæˆå¹¶ä¸‹è½½Wordæ–‡æ¡£
    if target_file and compare_file:
        word_file = generate_word_document(
            st.session_state.analysis_result,
            target_file.name,
            compare_file.name
        )
        
        if word_file:
            with open(word_file, "rb") as f:
                st.download_button(
                    label="ğŸ’¾ ä¸‹è½½åˆ†ææŠ¥å‘Š (Wordæ ¼å¼)",
                    data=f,
                    file_name=f"æ”¿ç­–åˆè§„æ€§åˆ†ææŠ¥å‘Š_{time.strftime('%Y%m%d')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(word_file)

# å¸®åŠ©ä¿¡æ¯
with st.expander("â„¹ï¸ ä½¿ç”¨å¸®åŠ©"):
    st.markdown("""
    1. é¦–å…ˆä¸Šä¼ ç›®æ ‡æ”¿ç­–æ–‡ä»¶ï¼ˆå·¦ä¾§ï¼‰å’Œå¾…æ¯”å¯¹æ–‡ä»¶ï¼ˆå³ä¾§ï¼‰
    2. ç‚¹å‡»"åŠ è½½åˆ†ææ¨¡å‹"æŒ‰é’®ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦å‡ åˆ†é’Ÿï¼‰
    3. æ¨¡å‹åŠ è½½å®Œæˆåï¼Œç‚¹å‡»"å¼€å§‹æ¯”å¯¹ä¸åˆè§„æ€§åˆ†æ"
    4. åˆ†æå®Œæˆåå¯ä»¥æŸ¥çœ‹ç»“æœå¹¶ä¸‹è½½WordæŠ¥å‘Š
    
    æ³¨æ„ï¼š
    - æ¨¡å‹åŠ è½½éœ€è¦ä¸€å®šæ—¶é—´å’Œèµ„æºï¼Œè¯·è€å¿ƒç­‰å¾…
    - ä¸ºä¿è¯åˆ†ææ•ˆæœï¼Œå»ºè®®ä¸Šä¼ æ¸…æ™°çš„PDFæ–‡ä»¶
    - åˆ†æç»“æœä»…ä¾›å‚è€ƒï¼Œé‡è¦å†³ç­–è¯·å’¨è¯¢ä¸“ä¸šäººå£«
    """)
