import streamlit as st
import docx
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
import re
import os
import tempfile
from datetime import datetime
from dashscope import Generation  # é˜¿é‡Œäº‘Qwenå¤§æ¨¡å‹SDK
import json
import fitz  # PyMuPDFç”¨äºPDFæ–‡æœ¬æå–
import pytesseract  # OCRè¯†åˆ«åº“
from PIL import Image
import io

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ“„ æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…· (Qwenå¢å¼ºç‰ˆ)")
st.write("ä¸Šä¼ åŸºå‡†æ–‡ä»¶å’Œå¾…æ¯”è¾ƒæ–‡ä»¶ï¼ˆæ”¯æŒWordå’ŒPDFï¼ŒåŒ…æ‹¬å›¾ç‰‡PDFï¼‰ï¼Œç³»ç»Ÿå°†ä½¿ç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½æ¡æ¬¾åŒ¹é…åˆ†æå¹¶ç”Ÿæˆåˆè§„æ€§æŠ¥å‘Šã€‚")

# Qwen APIå¯†é’¥é…ç½®
with st.sidebar:
    st.subheader("Qwenå¤§æ¨¡å‹é…ç½®")
    qwen_api_key = st.text_input("è¯·è¾“å…¥é˜¿é‡Œäº‘DashScope APIå¯†é’¥", type="password")
    if qwen_api_key:
        os.environ["DASHSCOPE_API_KEY"] = qwen_api_key
    st.info("éœ€è¦é˜¿é‡Œäº‘è´¦å·å’ŒDashScopeæœåŠ¡è®¿é—®æƒé™ï¼Œè·å–APIå¯†é’¥: https://dashscope.console.aliyun.com/")
    
    st.subheader("OCRé…ç½®")
    st.warning("å¤„ç†å›¾ç‰‡PDFéœ€è¦Tesseract OCRæ”¯æŒï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¹¶é…ç½®å¥½ç¯å¢ƒ")
    tess_path = st.text_input("Tesseract OCRè·¯å¾„", value=r"C:\Program Files\Tesseract-OCR\tesseract.exe")
    if tess_path:
        pytesseract.pytesseract.tesseract_cmd = tess_path

# è¾…åŠ©å‡½æ•°ï¼šä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼ˆæ”¯æŒWordå’ŒPDFï¼‰
def extract_text_from_file(file_path, file_type):
    if file_type == "docx":
        return extract_text_from_docx(file_path)
    elif file_type == "pdf":
        return extract_text_from_pdf(file_path)
    else:
        st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
        return ""

# ä»Wordæ–‡æ¡£æå–æ–‡æœ¬
def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        if para.text.strip():  # åªæ·»åŠ éç©ºæ®µè½
            full_text.append(para.text)
    return '\n'.join(full_text)

# ä»PDFæå–æ–‡æœ¬ï¼ˆæ”¯æŒå›¾ç‰‡PDFçš„OCRè¯†åˆ«ï¼‰
def extract_text_from_pdf(file_path):
    pdf_document = fitz.open(file_path)
    full_text = []
    
    for page_num in range(len(pdf_document)):
        page = pdf_document.load_page(page_num)
        text = page.get_text()
        
        # å¦‚æœé¡µé¢æ–‡æœ¬ä¸ºç©ºï¼Œå°è¯•OCRè¯†åˆ«å›¾ç‰‡å†…å®¹
        if not text.strip():
            st.info(f"PDFé¡µé¢ {page_num + 1} çœ‹èµ·æ¥æ˜¯å›¾ç‰‡ï¼Œæ­£åœ¨ä½¿ç”¨OCRæå–æ–‡æœ¬...")
            text = ocr_pdf_page(page)
        
        if text.strip():
            full_text.append(text)
    
    pdf_document.close()
    return '\n'.join(full_text)

# å¯¹PDFé¡µé¢è¿›è¡ŒOCRè¯†åˆ«
def ocr_pdf_page(page):
    # å°†PDFé¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡
    pix = page.get_pixmap(dpi=300)  # é«˜DPIæé«˜è¯†åˆ«ç²¾åº¦
    img = Image.open(io.BytesIO(pix.tobytes("png")))
    
    # ä½¿ç”¨Tesseractè¿›è¡ŒOCRè¯†åˆ«
    try:
        text = pytesseract.image_to_string(img, lang="chi_sim+eng")
        return text
    except Exception as e:
        st.error(f"OCRè¯†åˆ«å¤±è´¥: {str(e)}")
        st.warning("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…Tesseract OCRå¹¶é…ç½®äº†æ­£ç¡®è·¯å¾„")
        return ""

# è¾…åŠ©å‡½æ•°ï¼šä½¿ç”¨Qwenå¤§æ¨¡å‹æ‹†åˆ†æ¡æ¬¾
def split_terms_with_qwen(text):
    if not qwen_api_key:
        st.error("è¯·å…ˆé…ç½®Qwen APIå¯†é’¥")
        return []
    
    prompt = f"""è¯·å¸®æˆ‘ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ¡æ¬¾ï¼Œæ¯æ¡æ¡æ¬¾ä½œä¸ºä¸€ä¸ªç‹¬ç«‹é¡¹ã€‚æ¡æ¬¾é€šå¸¸ä»¥æ•°å­—ç¼–å·å¼€å¤´ï¼Œå¦‚"1. "ã€"2.1 "ç­‰ã€‚
    è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ¡æ¬¾çš„å®Œæ•´å†…å®¹ï¼ˆåŒ…å«ç¼–å·ï¼‰ã€‚å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ¡æ¬¾ç»“æ„ï¼ŒæŒ‰é€»è¾‘æ®µè½æ‹†åˆ†ã€‚
    
    æ–‡æœ¬å†…å®¹ï¼š
    {text[:2000]}  # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…è¶…å‡ºAPIé™åˆ¶
    """
    
    try:
        response = Generation.call(
            model="qwen-plus",
            prompt=prompt,
            result_format="json"
        )
        
        if response.status_code == 200:
            terms = json.loads(response.output.text)
            return terms if isinstance(terms, list) else []
        else:
            st.error(f"Qwen APIè°ƒç”¨å¤±è´¥: {response.message}")
            return []
    except Exception as e:
        st.error(f"æ¡æ¬¾æ‹†åˆ†å‡ºé”™: {str(e)}")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†
        return split_terms_with_regex(text)

# å¤‡ç”¨å‡½æ•°ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†æ¡æ¬¾
def split_terms_with_regex(text):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¡æ¬¾ç¼–å·ï¼Œå¦‚1. 1.1 2. ç­‰
    pattern = r'(\d+\.\s|\d+\.\d+\s|\d+\s)'
    terms = re.split(pattern, text)
    
    # é‡ç»„æ¡æ¬¾ï¼Œå°†ç¼–å·å’Œå†…å®¹åˆå¹¶
    result = []
    for i in range(1, len(terms), 2):
        if i + 1 < len(terms):
            term_number = terms[i].strip()
            term_content = terms[i+1].strip()
            result.append(f"{term_number} {term_content}")
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ¡æ¬¾ç¼–å·æ ¼å¼ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªæ¡æ¬¾
    if not result:
        result.append(text)
    
    return result

# è¾…åŠ©å‡½æ•°ï¼šä½¿ç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œæ¡æ¬¾åŒ¹é…å’Œåˆè§„æ€§åˆ†æ
def analyze_terms_with_qwen(benchmark_term, compare_terms):
    if not qwen_api_key:
        st.error("è¯·å…ˆé…ç½®Qwen APIå¯†é’¥")
        return None
    
    compare_text = "\n".join([f"[{i+1}] {term}" for i, term in enumerate(compare_terms)])
    
    prompt = f"""ä½œä¸ºæ³•å¾‹æ¡æ¬¾åˆè§„æ€§ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹åŸºå‡†æ¡æ¬¾ä¸å¾…æ¯”è¾ƒæ¡æ¬¾çš„åŒ¹é…åº¦å’Œåˆè§„æ€§ï¼š
    
    åŸºå‡†æ¡æ¬¾ï¼š
    {benchmark_term}
    
    å¾…æ¯”è¾ƒæ¡æ¬¾åˆ—è¡¨ï¼š
    {compare_text}
    
    è¯·æ‰¾å‡ºæœ€åŒ¹é…çš„æ¡æ¬¾ï¼Œå¹¶åˆ†æå…¶åˆè§„æ€§ã€‚å¦‚æœå­˜åœ¨åˆè§„é—®é¢˜ï¼Œè¯·æŒ‡å‡ºå…·ä½“å·®å¼‚å’Œä¸åˆè§„ä¹‹å¤„ã€‚
    è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    - best_match_index: æœ€åŒ¹é…æ¡æ¬¾çš„ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…é¡¹åˆ™ä¸º-1
    - similarity_score: åŒ¹é…åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰
    - compliance_analysis: åˆè§„æ€§åˆ†æè¯´æ˜
    - is_compliant: æ˜¯å¦åˆè§„ï¼ˆtrue/falseï¼‰
    """
    
    try:
        response = Generation.call(
            model="qwen-plus",
            prompt=prompt,
            result_format="json"
        )
        
        if response.status_code == 200:
            return json.loads(response.output.text)
        else:
            st.error(f"Qwen APIè°ƒç”¨å¤±è´¥: {response.message}")
            return None
    except Exception as e:
        st.error(f"æ¡æ¬¾åˆ†æå‡ºé”™: {str(e)}")
        return None

# ç”Ÿæˆåˆè§„æ€§æŠ¥å‘ŠWordæ–‡æ¡£
def generate_report(benchmark_terms, compare_terms, analysis_results):
    doc = docx.Document()
    
    # è®¾ç½®æ ‡é¢˜
    title = doc.add_heading("æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # æ·»åŠ æŠ¥å‘Šä¿¡æ¯
    doc.add_paragraph(f"ç”Ÿæˆæ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"åŸºå‡†æ¡æ¬¾æ•°é‡: {len(benchmark_terms)}")
    doc.add_paragraph(f"å¾…æ¯”è¾ƒæ¡æ¬¾æ•°é‡: {len(compare_terms)}")
    doc.add_paragraph("---")
    
    # æ·»åŠ åŒ¹é…æ¡æ¬¾åˆ†æ
    doc.add_heading("1. å¯åŒ¹é…æ¡æ¬¾åˆ†æ", level=1)
    matched_count = 0
    
    for i, (benchmark_term, analysis) in enumerate(zip(benchmark_terms, analysis_results)):
        if analysis and analysis.get('best_match_index', -1) != -1:
            matched_count += 1
            match_idx = analysis['best_match_index']
            match_term = compare_terms[match_idx]
            
            doc.add_heading(f"1.{matched_count} åŸºå‡†æ¡æ¬¾ {i+1}", level=2)
            p = doc.add_paragraph(benchmark_term)
            p.paragraph_format.space_after = Pt(12)
            
            doc.add_heading(f"1.{matched_count}.1 åŒ¹é…æ¡æ¬¾ {match_idx+1}", level=3)
            p = doc.add_paragraph(match_term)
            p.paragraph_format.space_after = Pt(12)
            
            doc.add_heading(f"1.{matched_count}.2 åˆè§„æ€§åˆ†æ", level=3)
            p = doc.add_paragraph(analysis['compliance_analysis'])
            p.paragraph_format.space_after = Pt(12)
            
            # æ·»åŠ åˆè§„æ€§æ ‡è¯†
            compliant_text = "åˆè§„" if analysis['is_compliant'] else "ä¸åˆè§„"
            compliant_color = "green" if analysis['is_compliant'] else "red"
            p = doc.add_paragraph(f"åˆè§„æ€§: {compliant_text} (åŒ¹é…åº¦: {analysis['similarity_score']}/100)")
            p.font.color.rgb = docx.shared.RGBColor.from_string(compliant_color)
            p.paragraph_format.space_after = Pt(24)
    
    # æ·»åŠ ä¸åˆè§„æ¡æ¬¾æ€»ç»“
    doc.add_heading("2. ä¸åˆè§„æ¡æ¬¾æ€»ç»“", level=1)
    
    non_compliant = [
        (i, term, analysis) 
        for i, (term, analysis) in enumerate(zip(benchmark_terms, analysis_results))
        if analysis and not analysis.get('is_compliant', False)
    ]
    
    if not non_compliant:
        doc.add_paragraph("æœªå‘ç°ä¸åˆè§„æ¡æ¬¾")
    else:
        for i, (term_idx, term, analysis) in enumerate(non_compliant):
            doc.add_heading(f"2.{i+1} åŸºå‡†æ¡æ¬¾ {term_idx+1}", level=2)
            p = doc.add_paragraph(term)
            p.paragraph_format.space_after = Pt(12)
            
            p = doc.add_paragraph(f"ä¸åˆè§„åŸå› : {analysis['compliance_analysis']}")
            p.paragraph_format.space_after = Pt(18)
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:
        doc.save(tmp.name)
        return tmp.name

# ä¸»ç¨‹åº
def main():
    # æ–‡ä»¶ä¸Šä¼ 
    col1, col2 = st.columns(2)
    
    with col1:
        benchmark_file = st.file_uploader("ä¸Šä¼ åŸºå‡†æ–‡ä»¶ (Wordæˆ–PDF)", type=["docx", "pdf"])
    
    with col2:
        compare_file = st.file_uploader("ä¸Šä¼ å¾…æ¯”è¾ƒæ–‡ä»¶ (Wordæˆ–PDF)", type=["docx", "pdf"])
    
    if benchmark_file and compare_file and st.button("å¼€å§‹åˆ†æ"):
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            with tempfile.TemporaryDirectory() as tmpdir:
                # ä¿å­˜åŸºå‡†æ–‡ä»¶
                benchmark_path = os.path.join(tmpdir, benchmark_file.name)
                with open(benchmark_path, "wb") as f:
                    f.write(benchmark_file.getvalue())
                
                # ä¿å­˜å¾…æ¯”è¾ƒæ–‡ä»¶
                compare_path = os.path.join(tmpdir, compare_file.name)
                with open(compare_path, "wb") as f:
                    f.write(compare_file.getvalue())
                
                # æå–æ–‡æœ¬
                benchmark_type = benchmark_file.name.split('.')[-1].lower()
                compare_type = compare_file.name.split('.')[-1].lower()
                
                benchmark_text = extract_text_from_file(benchmark_path, benchmark_type)
                compare_text = extract_text_from_file(compare_path, compare_type)
                
                # æ‹†åˆ†æ¡æ¬¾
                st.subheader("æ¡æ¬¾æå–ç»“æœ")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.info("æ­£åœ¨ä½¿ç”¨Qwenå¤§æ¨¡å‹æ‹†åˆ†åŸºå‡†æ¡æ¬¾...")
                    benchmark_terms = split_terms_with_qwen(benchmark_text)
                    st.success(f"æˆåŠŸæå–åŸºå‡†æ¡æ¬¾ {len(benchmark_terms)} æ¡")
                    with st.expander("æŸ¥çœ‹åŸºå‡†æ¡æ¬¾"):
                        for i, term in enumerate(benchmark_terms):
                            st.write(f"{i+1}. {term}")
                
                with col2:
                    st.info("æ­£åœ¨ä½¿ç”¨Qwenå¤§æ¨¡å‹æ‹†åˆ†å¾…æ¯”è¾ƒæ¡æ¬¾...")
                    compare_terms = split_terms_with_qwen(compare_text)
                    st.success(f"æˆåŠŸæå–å¾…æ¯”è¾ƒæ¡æ¬¾ {len(compare_terms)} æ¡")
                    with st.expander("æŸ¥çœ‹å¾…æ¯”è¾ƒæ¡æ¬¾"):
                        for i, term in enumerate(compare_terms):
                            st.write(f"{i+1}. {term}")
                
                # æ¡æ¬¾åŒ¹é…ä¸åˆ†æ
                if benchmark_terms and compare_terms:
                    st.subheader("æ¡æ¬¾åŒ¹é…ä¸åˆè§„æ€§åˆ†æ")
                    analysis_results = []
                    
                    progress_bar = st.progress(0)
                    for i, benchmark_term in enumerate(benchmark_terms):
                        st.info(f"æ­£åœ¨åˆ†æåŸºå‡†æ¡æ¬¾ {i+1}/{len(benchmark_terms)}...")
                        analysis = analyze_terms_with_qwen(benchmark_term, compare_terms)
                        analysis_results.append(analysis)
                        progress_bar.progress((i+1)/len(benchmark_terms))
                    
                    # æ˜¾ç¤ºåˆ†æç»“æœ
                    st.subheader("åˆ†æç»“æœæ‘˜è¦")
                    compliant_count = sum(1 for res in analysis_results if res and res.get('is_compliant', False))
                    st.metric("åˆè§„æ¡æ¬¾æ•°é‡", compliant_count)
                    st.metric("ä¸åˆè§„æ¡æ¬¾æ•°é‡", len(benchmark_terms) - compliant_count)
                    
                    # ç”Ÿæˆå¹¶æä¾›ä¸‹è½½æŠ¥å‘Š
                    st.subheader("ç”Ÿæˆåˆè§„æ€§æŠ¥å‘Š")
                    with st.spinner("æ­£åœ¨ç”ŸæˆWordæŠ¥å‘Š..."):
                        report_path = generate_report(benchmark_terms, compare_terms, analysis_results)
                        
                        with open(report_path, "rb") as f:
                            st.download_button(
                                label="ä¸‹è½½åˆè§„æ€§æŠ¥å‘Š",
                                data=f,
                                file_name=f"æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                            )

if __name__ == "__main__":
    main()
    
