import streamlit as st
import docx
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
import re
import os
import tempfile
from datetime import datetime
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import numpy as np
import io
import base64

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ“„ æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·")
st.write("ä¸Šä¼ åŸºå‡†æ–‡ä»¶å’Œå¾…æ¯”è¾ƒæ–‡ä»¶ï¼Œç³»ç»Ÿå°†è¿›è¡Œæ¡æ¬¾åŒ¹é…åˆ†æå¹¶ç”Ÿæˆåˆè§„æ€§æŠ¥å‘Šã€‚")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'api_key_valid' not in st.session_state:
    st.session_state.api_key_valid = False

# Qwen APIå¯†é’¥é…ç½®
with st.sidebar:
    st.subheader("Qwenå¤§æ¨¡å‹é…ç½®")
    qwen_api_key = st.text_input("è¯·è¾“å…¥é˜¿é‡Œäº‘DashScope APIå¯†é’¥", type="password")
    
    # éªŒè¯APIå¯†é’¥
    if qwen_api_key:
        os.environ["DASHSCOPE_API_KEY"] = qwen_api_key
        # ç®€å•éªŒè¯æ ¼å¼ï¼ˆå®é™…æœ‰æ•ˆæ€§éœ€è°ƒç”¨APIæ—¶æ‰çŸ¥é“ï¼‰
        if len(qwen_api_key) == 32 and qwen_api_key.startswith('sk-'):
            st.session_state.api_key_valid = True
            st.success("APIå¯†é’¥æ ¼å¼æœ‰æ•ˆ")
        else:
            st.session_state.api_key_valid = False
            st.warning("APIå¯†é’¥æ ¼å¼ä¼¼ä¹ä¸æ­£ç¡®ï¼Œåº”ä¸ºä»¥sk-å¼€å¤´çš„32ä½å­—ç¬¦ä¸²")
    else:
        st.session_state.api_key_valid = False
        st.info("éœ€è¦é˜¿é‡Œäº‘è´¦å·å’ŒDashScopeæœåŠ¡è®¿é—®æƒé™ï¼Œè·å–APIå¯†é’¥: https://dashscope.console.aliyun.com/")
        st.info("è‹¥æ— APIå¯†é’¥ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼è¿›è¡Œæ–‡æœ¬æ¯”å¯¹")

# æ£€æŸ¥Tesseractæ˜¯å¦å®‰è£…
def check_tesseract_installation():
    try:
        # å°è¯•è·å–Tesseractç‰ˆæœ¬ä¿¡æ¯
        pytesseract.get_tesseract_version()
        return True
    except pytesseract.TesseractNotFoundError:
        return False
    except Exception as e:
        st.error(f"Tesseractæ£€æŸ¥å‡ºé”™: {str(e)}")
        return False

# æ£€æŸ¥TesseractçŠ¶æ€å¹¶æç¤º
tesseract_available = check_tesseract_installation()
if not tesseract_available:
    with st.sidebar:
        st.warning("âš ï¸ æœªæ£€æµ‹åˆ°Tesseract OCRå¼•æ“ï¼Œå›¾ç‰‡å‹PDFå¤„ç†åŠŸèƒ½å°†å—é™")
        st.info("""
        å®‰è£…TesseractæŒ‡å—ï¼š
        1. ä¸‹è½½å®‰è£…åŒ…ï¼šhttps://github.com/UB-Mannheim/tesseract/wiki
        2. å®‰è£…æ—¶é€‰æ‹©ä¸­æ–‡è¯­è¨€åŒ…
        3. é…ç½®ç¯å¢ƒå˜é‡æˆ–åœ¨è®¾ç½®ä¸­æŒ‡å®šè·¯å¾„
        """)

# è¾…åŠ©å‡½æ•°ï¼šåˆ¤æ–­PDFé¡µé¢æ˜¯å¦åŒ…å«å¯é€‰æ–‡æœ¬
def has_selectable_text(page):
    text = page.get_text("text")
    # è¿‡æ»¤ç©ºç™½å­—ç¬¦åæ£€æŸ¥é•¿åº¦
    clean_text = re.sub(r'\s+', '', text)
    return len(clean_text) > 50  # è®¤ä¸º50ä¸ªä»¥ä¸Šéç©ºç™½å­—ç¬¦ä¸ºæœ‰æ•ˆæ–‡æœ¬

# è¾…åŠ©å‡½æ•°ï¼šä»PDFä¸­æå–æ–‡æœ¬ï¼ˆä¼˜å…ˆæ–‡æœ¬æå–ï¼Œå¿…è¦æ—¶OCRï¼‰
def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    full_text = []
    page_count = len(doc)
    
    with st.spinner(f"æ­£åœ¨è§£æPDFæ–‡ä»¶ï¼ˆå…±{page_count}é¡µï¼‰..."):
        progress_bar = st.progress(0)
        
        for i, page in enumerate(doc):
            # æ›´æ–°è¿›åº¦
            progress_bar.progress((i + 1) / page_count)
            
            # å…ˆå°è¯•æ–‡æœ¬æå–
            if has_selectable_text(page):
                text = page.get_text("text")
                full_text.append(f"[é¡µé¢{i+1} - æ–‡æœ¬æå–]\n{text}")
            else:
                # æ–‡æœ¬æå–å¤±è´¥ï¼Œå°è¯•OCR
                if not tesseract_available:
                    full_text.append(f"[é¡µé¢{i+1} - æ— æ³•å¤„ç†]\nè­¦å‘Šï¼šæœªå®‰è£…Tesseract OCRï¼Œæ— æ³•æå–å›¾ç‰‡ä¸­çš„æ–‡æœ¬å†…å®¹ã€‚")
                    continue
                
                try:
                    # å°†é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡
                    pix = page.get_pixmap(dpi=300)
                    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                    
                    # é¢„å¤„ç†ï¼šè½¬ä¸ºç°åº¦å¹¶äºŒå€¼åŒ–å¢å¼ºè¯†åˆ«ç‡
                    img_gray = img.convert('L')
                    threshold = 150  # é˜ˆå€¼å¯è°ƒæ•´
                    img_binary = img_gray.point(lambda p: p > threshold and 255)
                    
                    # è¿›è¡ŒOCRè¯†åˆ«ï¼ˆä¸­è‹±æ–‡ï¼‰
                    ocr_text = pytesseract.image_to_string(img_binary, lang="chi_sim+eng")
                    full_text.append(f"[é¡µé¢{i+1} - OCRè¯†åˆ«]\n{ocr_text}")
                except Exception as e:
                    full_text.append(f"[é¡µé¢{i+1} - å¤„ç†å¤±è´¥]\né”™è¯¯ï¼š{str(e)}")
        
        progress_bar.empty()
    
    return '\n\n'.join(full_text)

# è¾…åŠ©å‡½æ•°ï¼šä»docxæ–‡ä»¶ä¸­æå–æ–‡æœ¬
def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        if para.text.strip():  # åªæ·»åŠ éç©ºæ®µè½
            full_text.append(para.text)
    return '\n'.join(full_text)

# ç»Ÿä¸€çš„æ–‡ä»¶æå–å‡½æ•°
def extract_text_from_file(uploaded_file, file_type):
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_type}') as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_path = temp_file.name
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
        if file_type == 'pdf':
            text = extract_text_from_pdf(temp_path)
        elif file_type == 'docx':
            text = extract_text_from_docx(temp_path)
        else:
            text = ""
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_path)
        return text
    except Exception as e:
        st.error(f"æ–‡ä»¶å¤„ç†å‡ºé”™: {str(e)}")
        return ""

# ä¼˜åŒ–çš„ä¸­æ–‡æ¡æ¬¾æ‹†åˆ†å‡½æ•°
def split_chinese_terms(text):
    """æ‹†åˆ†ä¸­æ–‡æ¡æ¬¾ï¼Œæ”¯æŒå¤šç§ç¼–å·æ ¼å¼ï¼Œå¢åŠ ç©ºå€¼å’Œå¼‚å¸¸å¤„ç†"""
    # é¦–å…ˆæ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
    if not text or not isinstance(text, str):
        st.warning("è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–æ— æ•ˆï¼Œæ— æ³•è¿›è¡Œæ¡æ¬¾æ‹†åˆ†")
        return []
    
    # æ¸…é™¤å¤šä½™ç©ºè¡Œå’Œç©ºæ ¼
    text = re.sub(r'\n+', '\n', text.strip())
    
    # ä¸­æ–‡æ¡æ¬¾å¸¸è§çš„ç¼–å·æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
    patterns = [
        r'(\d+\.\s+)',                # 1. 
        r'(\d+\.\d+\s+)',             # 1.1 
        r'(\(\d+\)\s+)',              # (1) 
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\ã€\s+)',  # ä¸€ã€ 
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]æ¡\s+)',   # ç¬¬ä¸€æ¡
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]æ¬¾\s+)',   # ç¬¬ä¸€æ¬¾
        r'(\d+\)\s+)',                # 1)
        r'([A-Za-z]\.\s+)',           # A. 
    ]
    
    # ç»„åˆæ‰€æœ‰æ¨¡å¼
    combined_pattern = '|'.join(patterns)
    
    # æ‹†åˆ†æ–‡æœ¬
    parts = re.split(combined_pattern, text)
    
    terms = []
    current_term = ""
    
    for part in parts:
        # è·³è¿‡ç©ºå€¼æˆ–ä»…å«ç©ºç™½å­—ç¬¦çš„éƒ¨åˆ†
        if not part or not part.strip():
            continue
            
        # æ£€æŸ¥å½“å‰éƒ¨åˆ†æ˜¯å¦ä¸ºæ¡æ¬¾ç¼–å·
        is_numbering = any(re.fullmatch(pattern.strip(), part.strip()) for pattern in patterns)
        
        if is_numbering:
            # å¦‚æœå·²æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜å½“å‰æ¡æ¬¾
            if current_term.strip():
                terms.append(current_term.strip())
            # å¼€å§‹æ–°æ¡æ¬¾
            current_term = part
        else:
            # ç´¯åŠ æ¡æ¬¾å†…å®¹
            current_term += part
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ¡æ¬¾
    if current_term.strip():
        terms.append(current_term.strip())
    
    # æ¡æ¬¾æ‹†åˆ†æ•ˆæœè¯„ä¼°
    if len(terms) < 3 and len(text) > 500:
        st.info(f"æ£€æµ‹åˆ°å¯èƒ½çš„æ¡æ¬¾æ‹†åˆ†æ•ˆæœä¸ä½³ï¼ˆå…±{len(terms)}æ¡ï¼‰ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
    
    return terms

# åŸºç¡€æ¨¡å¼çš„æ¡æ¬¾åŒ¹é…ï¼ˆæ— APIå¯†é’¥æ—¶ä½¿ç”¨ï¼‰
def basic_term_matching(benchmark_term, compare_terms):
    """ç®€å•çš„åŸºäºå…³é”®è¯çš„æ¡æ¬¾åŒ¹é…"""
    best_match = None
    best_score = 0
    
    # æå–åŸºå‡†æ¡æ¬¾å…³é”®è¯
    bench_words = set(re.findall(r'[\u4e00-\u9fff]+', benchmark_term))  # æå–ä¸­æ–‡å­—ç¬¦
    bench_words.update(re.findall(r'\b[a-zA-Z]+\b', benchmark_term))  # æå–è‹±æ–‡å­—ç¬¦
    bench_words = [w for w in bench_words if len(w) > 1]  # è¿‡æ»¤å•å­—
    
    if not bench_words:
        return None, 0
    
    for term in compare_terms:
        # æå–å¯¹æ¯”æ¡æ¬¾å…³é”®è¯
        term_words = set(re.findall(r'[\u4e00-\u9fff]+', term))
        term_words.update(re.findall(r'\b[a-zA-Z]+\b', term))
        term_words = [w for w in term_words if len(w) > 1]
        
        if not term_words:
            continue
            
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆäº¤é›†/å¹¶é›†ï¼‰
        common = len(bench_words & term_words)
        total = len(bench_words | term_words)
        score = common / total if total > 0 else 0
        
        if score > best_score:
            best_score = score
            best_match = term
    
    return best_match, best_score

# ç”ŸæˆWordæŠ¥å‘Š
def generate_word_report(benchmark_name, compare_results):
    doc = docx.Document()
    
    # æ ‡é¢˜
    title = doc.add_heading('æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # åŸºæœ¬ä¿¡æ¯
    doc.add_paragraph(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"åŸºå‡†æ–‡ä»¶: {benchmark_name}")
    doc.add_paragraph("")
    
    # ç›®å½•
    doc.add_heading('ç›®å½•', level=1)
    for i, (file_name, _) in enumerate(compare_results.items()):
        doc.add_paragraph(f"{i+1}. {file_name}", style='List Number')
    doc.add_paragraph("")
    
    # è¯¦ç»†ç»“æœ
    for file_name, result in compare_results.items():
        doc.add_heading(f"æ–‡ä»¶: {file_name}", level=1)
        
        # å¯åŒ¹é…æ¡æ¬¾
        doc.add_heading("å¯åŒ¹é…æ¡æ¬¾", level=2)
        if result['matched']:
            for idx, item in enumerate(result['matched'], 1):
                doc.add_heading(f"åŒ¹é…é¡¹ {idx} (ç›¸ä¼¼åº¦: {item['score']:.2f})", level=3)
                
                p = doc.add_paragraph("åŸºå‡†æ¡æ¬¾: ")
                p.add_run(item['benchmark']).bold = True
                
                p = doc.add_paragraph("å¯¹æ¯”æ¡æ¬¾: ")
                p.add_run(item['compare']).bold = True
                
                if 'analysis' in item:
                    doc.add_paragraph(f"åˆ†æ: {item['analysis']}")
        else:
            doc.add_paragraph("æœªæ‰¾åˆ°å¯åŒ¹é…çš„æ¡æ¬¾")
        
        # ä¸åˆè§„æ¡æ¬¾
        doc.add_heading("ä¸åˆè§„æ¡æ¬¾æ€»ç»“", level=2)
        if result['non_compliant']:
            for idx, term in enumerate(result['non_compliant'], 1):
                doc.add_paragraph(f"{idx}. {term}", style='List Number')
        else:
            doc.add_paragraph("æœªå‘ç°ä¸åˆè§„æ¡æ¬¾")
        
        doc.add_page_break()
    
    # ä¿å­˜åˆ°å†…å­˜
    buffer = io.BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    
    return buffer

# ä¸»å‡½æ•°
def main():
    # ä¸Šä¼ æ–‡ä»¶
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("åŸºå‡†æ–‡ä»¶")
        benchmark_file = st.file_uploader("ä¸Šä¼ åŸºå‡†æ–‡ä»¶ (PDFæˆ–DOCX)", type=['pdf', 'docx'], key='benchmark')
    
    with col2:
        st.subheader("å¯¹æ¯”æ–‡ä»¶")
        compare_files = st.file_uploader(
            "ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªå¯¹æ¯”æ–‡ä»¶ (PDFæˆ–DOCX)", 
            type=['pdf', 'docx'], 
            key='compare',
            accept_multiple_files=True
        )
    
    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆ†æ", disabled=not (benchmark_file and compare_files)):
        # æå–åŸºå‡†æ–‡ä»¶æ–‡æœ¬å’Œæ¡æ¬¾
        with st.spinner("æ­£åœ¨å¤„ç†åŸºå‡†æ–‡ä»¶..."):
            bench_type = benchmark_file.name.split('.')[-1].lower()
            bench_text = extract_text_from_file(benchmark_file, bench_type)
            
            if not bench_text:
                st.error("æ— æ³•ä»åŸºå‡†æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹")
                return
                
            st.success(f"åŸºå‡†æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæå–åˆ°æ–‡æœ¬é•¿åº¦: {len(bench_text)}å­—ç¬¦")
            bench_terms = split_chinese_terms(bench_text)
            st.info(f"ä»åŸºå‡†æ–‡ä»¶ä¸­æ‹†åˆ†å‡º {len(bench_terms)} æ¡æ¡æ¬¾")
        
        # å¤„ç†æ¯ä¸ªå¯¹æ¯”æ–‡ä»¶
        compare_results = {}
        use_advanced = st.session_state.api_key_valid
        
        if use_advanced:
            st.info("å°†ä½¿ç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œé«˜çº§æ¡æ¬¾åŒ¹é…åˆ†æ")
        else:
            st.info("æœªæ£€æµ‹åˆ°æœ‰æ•ˆAPIå¯†é’¥ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å¼è¿›è¡Œæ¡æ¬¾åŒ¹é…")
        
        # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
        progress_bar = st.progress(0)
        total_files = len(compare_files)
        
        for file_idx, compare_file in enumerate(compare_files, 1):
            st.subheader(f"æ­£åœ¨å¤„ç†: {compare_file.name}")
            
            # æå–å¯¹æ¯”æ–‡ä»¶æ–‡æœ¬å’Œæ¡æ¬¾
            with st.spinner(f"æå–æ–‡æœ¬å’Œæ‹†åˆ†æ¡æ¬¾..."):
                comp_type = compare_file.name.split('.')[-1].lower()
                comp_text = extract_text_from_file(compare_file, comp_type)
                
                if not comp_text:
                    st.warning(f"æ— æ³•ä» {compare_file.name} ä¸­æå–æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
                    progress_bar.progress(file_idx / total_files)
                    continue
                    
                comp_terms = split_chinese_terms(comp_text)
                st.info(f"ä» {compare_file.name} ä¸­æ‹†åˆ†å‡º {len(comp_terms)} æ¡æ¡æ¬¾")
            
            # æ¡æ¬¾åŒ¹é…åˆ†æ
            matched_terms = []
            comp_terms_used = set()  # è·Ÿè¸ªå·²åŒ¹é…çš„æ¡æ¬¾
            
            with st.spinner(f"æ­£åœ¨è¿›è¡Œæ¡æ¬¾åŒ¹é…åˆ†æ..."):
                for bench_idx, bench_term in enumerate(bench_terms[:20]):  # é™åˆ¶å‰20æ¡ä»¥æé«˜æ•ˆç‡
                    # æ˜¾ç¤ºå½“å‰è¿›åº¦
                    if len(bench_terms) > 0:
                        sub_progress = (bench_idx + 1) / len(bench_terms)
                        st.progress(sub_progress, text=f"å¤„ç†æ¡æ¬¾ {bench_idx + 1}/{len(bench_terms)}")
                    
                    # æŸ¥æ‰¾æœ€ä½³åŒ¹é…
                    if use_advanced:
                        # è¿™é‡Œåº”è¯¥æ˜¯è°ƒç”¨Qwenå¤§æ¨¡å‹çš„ä»£ç 
                        # ä¸ºäº†é¿å…é”™è¯¯ï¼Œå½“APIä¸å¯ç”¨æ—¶ä½¿ç”¨åŸºç¡€æ¨¡å¼
                        try:
                            from dashscope import Generation
                            
                            prompt = f"""
                            è¯·å¯¹æ¯”ä»¥ä¸‹ä¸¤ä¸ªæ¡æ¬¾çš„å†…å®¹ï¼Œå¹¶åˆ¤æ–­å®ƒä»¬çš„åŒ¹é…ç¨‹åº¦ï¼ˆ0-100åˆ†ï¼‰ã€‚
                            åŒæ—¶åˆ†æå®ƒä»¬çš„ç›¸åŒç‚¹å’Œä¸åŒç‚¹ï¼Œå¹¶ç»™å‡ºåˆè§„æ€§åˆ¤æ–­ã€‚
                            
                            åŸºå‡†æ¡æ¬¾: {bench_term[:200]}
                            
                            è¯·ä»ä»¥ä¸‹å¯¹æ¯”æ¡æ¬¾ä¸­æ‰¾åˆ°æœ€åŒ¹é…çš„ä¸€é¡¹:
                            {chr(10).join([f"{i+1}. {t[:100]}..." for i, t in enumerate(comp_terms)])}
                            
                            è¯·ä»¥JSONæ ¼å¼è¿”å›:
                            {{
                                "best_match_index": æœ€åŒ¹é…æ¡æ¬¾çš„ç´¢å¼•(ä»0å¼€å§‹),
                                "similarity_score": åŒ¹é…åº¦(0-100),
                                "analysis": "ç›¸åŒç‚¹å’Œä¸åŒç‚¹åˆ†æï¼Œä»¥åŠåˆè§„æ€§åˆ¤æ–­"
                            }}
                            """
                            
                            response = Generation.call(
                                model="qwen-plus",
                                prompt=prompt,
                                result_format="json"
                            )
                            
                            if response.status_code == 200:
                                try:
                                    analysis_result = json.loads(response.output.text)
                                    match_idx = analysis_result.get("best_match_index", -1)
                                    score = analysis_result.get("similarity_score", 0) / 100  # è½¬æ¢ä¸º0-1èŒƒå›´
                                    analysis = analysis_result.get("analysis", "")
                                    
                                    if 0 <= match_idx < len(comp_terms) and match_idx not in comp_terms_used:
                                        comp_terms_used.add(match_idx)
                                        matched_terms.append({
                                            "benchmark": bench_term,
                                            "compare": comp_terms[match_idx],
                                            "score": score,
                                            "analysis": analysis
                                        })
                                except:
                                    # è§£æç»“æœå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼
                                    best_match, score = basic_term_matching(bench_term, comp_terms)
                                    if best_match:
                                        matched_terms.append({
                                            "benchmark": bench_term,
                                            "compare": best_match,
                                            "score": score
                                        })
                            else:
                                # APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼
                                best_match, score = basic_term_matching(bench_term, comp_terms)
                                if best_match:
                                    matched_terms.append({
                                        "benchmark": bench_term,
                                        "compare": best_match,
                                        "score": score
                                    })
                        except Exception as e:
                            st.warning(f"é«˜çº§åˆ†æå‡ºé”™ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼: {str(e)}")
                            best_match, score = basic_term_matching(bench_term, comp_terms)
                            if best_match:
                                matched_terms.append({
                                    "benchmark": bench_term,
                                    "compare": best_match,
                                    "score": score
                                })
                    else:
                        # ä½¿ç”¨åŸºç¡€æ¨¡å¼
                        best_match, score = basic_term_matching(bench_term, comp_terms)
                        if best_match:
                            matched_terms.append({
                                "benchmark": bench_term,
                                "compare": best_match,
                                "score": score
                            })
            
            # ç­›é€‰å‡ºåŒ¹é…åº¦é«˜çš„æ¡æ¬¾ï¼ˆ>0.7ï¼‰
            valid_matches = [m for m in matched_terms if m['score'] > 0.7]
            valid_matches.sort(key=lambda x: x['score'], reverse=True)
            
            # æ‰¾å‡ºæœªåŒ¹é…çš„æ¡æ¬¾ï¼ˆä¸åˆè§„ï¼‰
            non_compliant = [comp_terms[i] for i in range(len(comp_terms)) if i not in comp_terms_used]
            
            # ä¿å­˜ç»“æœ
            compare_results[compare_file.name] = {
                "matched": valid_matches,
                "non_compliant": non_compliant[:10]  # é™åˆ¶æ˜¾ç¤ºå‰10æ¡
            }
            
            # æ›´æ–°æ€»ä½“è¿›åº¦
            progress_bar.progress(file_idx / total_files)
        
        progress_bar.empty()
        
        # æ˜¾ç¤ºç»“æœ
        st.success("æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæˆï¼")
        
        # åˆ›å»ºç»“æœæ ‡ç­¾é¡µ
        tabs = st.tabs([f"ğŸ“„ {name}" for name in compare_results.keys()])
        
        for tab, (file_name, result) in zip(tabs, compare_results.items()):
            with tab:
                st.header(f"æ–‡ä»¶: {file_name}")
                
                # æ˜¾ç¤ºåŒ¹é…æ¡æ¬¾
                st.subheader("å¯åŒ¹é…æ¡æ¬¾")
                if result['matched']:
                    for i, item in enumerate(result['matched']):
                        with st.expander(f"åŒ¹é…é¡¹ {i+1} (ç›¸ä¼¼åº¦: {item['score']:.2f})"):
                            col_a, col_b = st.columns(2)
                            with col_a:
                                st.markdown("**åŸºå‡†æ¡æ¬¾:**")
                                st.write(item['benchmark'])
                            with col_b:
                                st.markdown("**å¯¹æ¯”æ¡æ¬¾:**")
                                st.write(item['compare'])
                            if 'analysis' in item:
                                st.markdown("**åˆ†æ:**")
                                st.write(item['analysis'])
                else:
                    st.info("æœªæ‰¾åˆ°å¯åŒ¹é…çš„æ¡æ¬¾")
                
                # æ˜¾ç¤ºä¸åˆè§„æ¡æ¬¾
                st.subheader("ä¸åˆè§„æ¡æ¬¾æ€»ç»“")
                if result['non_compliant']:
                    for i, term in enumerate(result['non_compliant']):
                        st.write(f"{i+1}. {term[:200]}...")  # æ˜¾ç¤ºå‰200å­—ç¬¦
                else:
                    st.success("æœªå‘ç°ä¸åˆè§„æ¡æ¬¾")
        
        # ç”Ÿæˆå¹¶æä¾›ä¸‹è½½æŠ¥å‘Š
        st.subheader("ç”ŸæˆæŠ¥å‘Š")
        report_buffer = generate_word_report(benchmark_file.name, compare_results)
        
        # æä¾›ä¸‹è½½
        b64 = base64.b64encode(report_buffer.getvalue()).decode()
        href = f'<a href="data:application/octet-stream;base64,{b64}" download="æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š_{datetime.now().strftime("%Y%m%d")}.docx">ä¸‹è½½WordæŠ¥å‘Š</a>'
        st.markdown(href, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
    
