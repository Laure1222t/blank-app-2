import streamlit as st
import docx
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
import re
import os
import tempfile
from datetime import datetime
from dashscope import Generation
import json
import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import numpy as np
import io
import base64

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ“„ æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·")
st.write("ä¸Šä¼ åŸºå‡†æ–‡ä»¶å’Œå¤šä¸ªå¯¹æ¯”æ–‡ä»¶ï¼Œç³»ç»Ÿå°†åˆ†ææ¡æ¬¾åŒ¹é…æƒ…å†µå¹¶ç”Ÿæˆåˆè§„æ€§æŠ¥å‘Šã€‚")

# æ£€æŸ¥Tesseractæ˜¯å¦å®‰è£…
def check_tesseract_installation():
    try:
        # å°è¯•è¿è¡Œtesseractå‘½ä»¤
        pytesseract.get_tesseract_version()
        return True
    except Exception:
        return False

# é…ç½®Tesseractè·¯å¾„ï¼ˆé’ˆå¯¹æœ¬åœ°è¿è¡Œï¼‰
def configure_tesseract():
    if not check_tesseract_installation():
        with st.sidebar:
            st.warning("æœªæ£€æµ‹åˆ°Tesseract OCRå¼•æ“ï¼Œå›¾ç‰‡å‹PDFå°†æ— æ³•å¤„ç†")
            st.info("""
            å®‰è£…æŒ‡å—ï¼š
            1. ä¸‹è½½å®‰è£…Tesseract: https://github.com/UB-Mannheim/tesseract/wiki
            2. å®‰è£…æ—¶å‹¾é€‰ä¸­æ–‡è¯­è¨€åŒ…
            3. åœ¨ä¸‹æ–¹è¾“å…¥å®‰è£…è·¯å¾„ï¼ˆå¦‚C:\\Program Files\\Tesseract-OCR\\tesseract.exeï¼‰
            """)
            tesseract_path = st.text_input("Tesseractå®‰è£…è·¯å¾„")
            if tesseract_path:
                try:
                    pytesseract.pytesseract.tesseract_cmd = tesseract_path
                    if check_tesseract_installation():
                        st.success("Tesseracté…ç½®æˆåŠŸ")
                except Exception as e:
                    st.error(f"é…ç½®å¤±è´¥: {str(e)}")
    return check_tesseract_installation()

# Qwen APIå¯†é’¥é…ç½®
with st.sidebar:
    st.subheader("Qwenå¤§æ¨¡å‹é…ç½®")
    qwen_api_key = st.text_input("è¯·è¾“å…¥é˜¿é‡Œäº‘DashScope APIå¯†é’¥", type="password")
    if qwen_api_key:
        os.environ["DASHSCOPE_API_KEY"] = qwen_api_key
    st.info("éœ€è¦é˜¿é‡Œäº‘è´¦å·å’ŒDashScopeæœåŠ¡è®¿é—®æƒé™ï¼Œè·å–APIå¯†é’¥: https://dashscope.console.aliyun.com/")
    
    # é…ç½®Tesseract
    tesseract_available = configure_tesseract()

# æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«å¯é€‰æ‹©çš„æ–‡æœ¬
def has_selectable_text(page):
    text = page.get_text().strip()
    # å¦‚æœæ–‡æœ¬é•¿åº¦å¤§äº50ä¸ªå­—ç¬¦ï¼Œè®¤ä¸ºæ˜¯å¯é€‰æ‹©çš„æ–‡æœ¬
    return len(text) > 50

# ä»PDFä¸­æå–æ–‡æœ¬ï¼ˆä¼˜å…ˆæ–‡æœ¬æå–ï¼Œå¿…è¦æ—¶ä½¿ç”¨OCRï¼‰
def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    full_text = []
    tesseract_available = check_tesseract_installation()
    
    with st.spinner("æ­£åœ¨æå–PDFå†…å®¹..."):
        progress_bar = st.progress(0)
        for i, page in enumerate(doc):
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯é€‰æ‹©çš„æ–‡æœ¬
            if has_selectable_text(page):
                text = page.get_text().strip()
                full_text.append(f"[æ–‡æœ¬æå–] ç¬¬{i+1}é¡µ:\n{text}")
            else:
                # æ²¡æœ‰å¯é€‰æ‹©çš„æ–‡æœ¬ï¼Œå°è¯•OCR
                if tesseract_available:
                    with st.spinner(f"æ­£åœ¨å¯¹ç¬¬{i+1}é¡µè¿›è¡ŒOCRè¯†åˆ«..."):
                        # å°†é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡
                        pix = page.get_pixmap(dpi=300)
                        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                        
                        # é¢„å¤„ç†ï¼šè½¬ä¸ºç°åº¦å¹¶äºŒå€¼åŒ–
                        img_gray = img.convert('L')
                        img_np = np.array(img_gray)
                        thresh = 150  # é˜ˆå€¼è°ƒæ•´
                        img_binary = (img_np > thresh) * 255
                        img_processed = Image.fromarray(img_binary.astype(np.uint8))
                        
                        # è¿›è¡ŒOCRè¯†åˆ«ï¼Œæ”¯æŒä¸­è‹±æ–‡
                        ocr_text = pytesseract.image_to_string(
                            img_processed, 
                            lang="chi_sim+eng"
                        ).strip()
                        
                        full_text.append(f"[OCRè¯†åˆ«] ç¬¬{i+1}é¡µ:\n{ocr_text}")
                else:
                    full_text.append(f"[æ— æ³•è¯†åˆ«] ç¬¬{i+1}é¡µ: æœªå®‰è£…Tesseract OCRï¼Œæ— æ³•å¤„ç†å›¾ç‰‡å‹PDFå†…å®¹")
            
            progress_bar.progress((i + 1) / len(doc))
    
    return '\n\n'.join(full_text)

# ä»docxæ–‡ä»¶ä¸­æå–æ–‡æœ¬
def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        if para.text.strip():  # åªæ·»åŠ éç©ºæ®µè½
            full_text.append(para.text)
    return '\n'.join(full_text)

# ç»Ÿä¸€çš„æ–‡ä»¶æ–‡æœ¬æå–å‡½æ•°
def extract_text_from_file(uploaded_file, file_type):
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_type}') as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_path = temp_file.name
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬
        if file_type == 'pdf':
            text = extract_text_from_pdf(temp_path)
        elif file_type in ['docx', 'doc']:  # ç®€å•å¤„ç†ï¼Œå®é™…docéœ€è¦é¢å¤–åº“
            text = extract_text_from_docx(temp_path)
        else:
            text = ""
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_path)
        return text
    except Exception as e:
        st.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}")
        return ""

# ä¼˜åŒ–çš„ä¸­æ–‡æ¡æ¬¾æ‹†åˆ†å‡½æ•°
def split_chinese_terms(text):
    """æ‹†åˆ†ä¸­æ–‡æ¡æ¬¾ï¼Œæ”¯æŒå¤šç§ç¼–å·æ ¼å¼ï¼Œå¢åŠ ç©ºå€¼å’Œå¼‚å¸¸å¤„ç†"""
    # é¦–å…ˆæ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰æ•ˆ
    if not text or not isinstance(text, str):
        st.warning("è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–æ— æ•ˆï¼Œæ— æ³•è¿›è¡Œæ¡æ¬¾æ‹†åˆ†")
        return []
    
    # æ¸…é™¤å¤šä½™ç©ºè¡Œå’Œç©ºæ ¼
    text = re.sub(r'\n+', '\n', text.strip())
    
    # ä¸­æ–‡æ¡æ¬¾å¸¸è§çš„ç¼–å·æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
    patterns = [
        r'(\d+\.\s+)',                # 1. 
        r'(\d+\.\d+\s+)',             # 1.1 
        r'(\(\d+\)\s+)',              # (1) 
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\ã€\s+)',  # ä¸€ã€ 
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]æ¡\s+)',   # ç¬¬ä¸€æ¡
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]æ¬¾\s+)',   # ç¬¬ä¸€æ¬¾
        r'(\d+\)\s+)',                # 1)
        r'([A-Za-z]\.\s+)',           # A. 
    ]
    
    # ç»„åˆæ‰€æœ‰æ¨¡å¼
    combined_pattern = '|'.join(patterns)
    
    # æ‹†åˆ†æ–‡æœ¬
    parts = re.split(combined_pattern, text)
    
    terms = []
    current_term = ""
    
    for part in parts:
        # è·³è¿‡ç©ºå€¼æˆ–ä»…å«ç©ºç™½å­—ç¬¦çš„éƒ¨åˆ†
        if not part or not part.strip():
            continue
            
        # æ£€æŸ¥å½“å‰éƒ¨åˆ†æ˜¯å¦ä¸ºæ¡æ¬¾ç¼–å·
        is_numbering = any(re.fullmatch(pattern.strip(), part.strip()) for pattern in patterns)
        
        if is_numbering:
            # å¦‚æœå·²æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜å½“å‰æ¡æ¬¾
            if current_term.strip():
                terms.append(current_term.strip())
            # å¼€å§‹æ–°æ¡æ¬¾
            current_term = part
        else:
            # ç´¯åŠ æ¡æ¬¾å†…å®¹
            current_term += part
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ¡æ¬¾
    if current_term.strip():
        terms.append(current_term.strip())
    
    # å¦‚æœæ¡æ¬¾æ•°é‡è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯æ‹†åˆ†æ•ˆæœä¸å¥½ï¼Œæç¤ºç”¨æˆ·
    if len(terms) < 3 and len(text) > 500:
        st.info(f"æ£€æµ‹åˆ°å¯èƒ½çš„æ¡æ¬¾æ‹†åˆ†æ•ˆæœä¸ä½³ï¼ˆå…±{len(terms)}æ¡ï¼‰ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
    
    return terms

# ä½¿ç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œæ¡æ¬¾åŒ¹é…å’Œåˆè§„æ€§åˆ†æ
def analyze_terms_with_qwen(benchmark_term, compare_terms):
    if not qwen_api_key:
        st.error("è¯·å…ˆé…ç½®Qwen APIå¯†é’¥")
        return None, "æœªé…ç½®APIå¯†é’¥"
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ¡æ¬¾åˆè§„æ€§åˆ†æä¸“å®¶ã€‚è¯·åˆ†æå¯¹æ¯”æ¡æ¬¾ä¸åŸºå‡†æ¡æ¬¾çš„åŒ¹é…ç¨‹åº¦å’Œå·®å¼‚ã€‚
    åŸºå‡†æ¡æ¬¾: {benchmark_term}
    
    å¾…æ¯”è¾ƒæ¡æ¬¾åˆ—è¡¨:
    {chr(10).join([f"{i+1}. {term}" for i, term in enumerate(compare_terms)])}
    
    è¯·å…ˆåˆ¤æ–­å“ªä¸ªå¾…æ¯”è¾ƒæ¡æ¬¾ä¸åŸºå‡†æ¡æ¬¾æœ€åŒ¹é…ï¼Œç„¶ååˆ†æå®ƒä»¬çš„å·®å¼‚ã€‚
    è¾“å‡ºæ ¼å¼è¦æ±‚:
    1. åŒ¹é…æ¡æ¬¾ç¼–å·: [æ•°å­—ï¼Œå¦‚1è¡¨ç¤ºç¬¬ä¸€ä¸ªå¾…æ¯”è¾ƒæ¡æ¬¾]
    2. åŒ¹é…åº¦: [0-100çš„æ•°å­—ï¼Œè¡¨ç¤ºåŒ¹é…ç™¾åˆ†æ¯”]
    3. ç›¸åŒç‚¹: [ç®€è¦æè¿°ç›¸åŒå†…å®¹]
    4. å·®å¼‚ç‚¹: [ç®€è¦æè¿°ä¸åŒå†…å®¹]
    5. åˆè§„æ€§åˆ¤æ–­: [åˆè§„/éƒ¨åˆ†åˆè§„/ä¸åˆè§„]
    6. ç†ç”±: [è¯´æ˜åˆ¤æ–­ä¾æ®]
    
    è¯·ç”¨ä¸­æ–‡è¾“å‡ºï¼Œç¡®ä¿ç»“æœç®€æ´æ˜äº†ã€‚
    """
    
    try:
        response = Generation.call(
            model="qwen-plus",
            prompt=prompt
        )
        
        if response.status_code == 200:
            result = response.output.text
            # æå–åŒ¹é…åº¦
            match_score = re.search(r'åŒ¹é…åº¦: (\d+)', result)
            score = int(match_score.group(1)) if match_score else 0
            return score, result
        else:
            st.error(f"Qwen APIè°ƒç”¨å¤±è´¥: {response.message}")
            return 0, f"APIè°ƒç”¨å¤±è´¥: {response.message}"
    except Exception as e:
        st.error(f"åˆ†æå‡ºé”™: {str(e)}")
        return 0, f"åˆ†æå‡ºé”™: {str(e)}"

# ç”ŸæˆWordæŠ¥å‘Š
def generate_word_report(benchmark_name, compare_results):
    doc = docx.Document()
    
    # æ·»åŠ æ ‡é¢˜
    title = doc.add_heading("æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # æ·»åŠ æŠ¥å‘Šä¿¡æ¯
    doc.add_paragraph(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph(f"åŸºå‡†æ–‡ä»¶: {benchmark_name}")
    doc.add_paragraph(f"å¯¹æ¯”æ–‡ä»¶æ•°é‡: {len(compare_results)}")
    doc.add_page_break()
    
    # æ·»åŠ ç›®å½•
    doc.add_heading("ç›®å½•", 1)
    for i, (file_name, _) in enumerate(compare_results.items(), 1):
        para = doc.add_paragraph(f"{i}. {file_name}", style='List Number')
        para.hyperlink = f"#{file_name}"  # ç®€å•çš„ç›®å½•é“¾æ¥æ ‡è®°
    
    doc.add_page_break()
    
    # ä¸ºæ¯ä¸ªå¯¹æ¯”æ–‡ä»¶æ·»åŠ åˆ†æç»“æœ
    for file_name, analysis in compare_results.items():
        # æ–‡ä»¶æ ‡é¢˜
        heading = doc.add_heading(file_name, 1)
        heading.paragraph_format.keep_with_next = True
        
        # åŒ¹é…çš„æ¡æ¬¾
        doc.add_heading("å¯åŒ¹é…æ¡æ¬¾", 2)
        matched_terms = [t for t in analysis if t['score'] >= 70]
        
        if matched_terms:
            for term in matched_terms:
                doc.add_heading(f"åŸºå‡†æ¡æ¬¾: {term['benchmark_term'][:30]}...", 3)
                doc.add_paragraph(f"åŒ¹é…æ¡æ¬¾: {term['matched_term'][:50]}...")
                doc.add_paragraph(f"åŒ¹é…åº¦: {term['score']}%")
                doc.add_paragraph("åˆ†æ:")
                doc.add_paragraph(term['analysis'], style='List Bullet')
                doc.add_paragraph("")
        else:
            doc.add_paragraph("æœªå‘ç°å¯åŒ¹é…çš„æ¡æ¬¾")
        
        # ä¸åˆè§„çš„æ¡æ¬¾
        doc.add_heading("ä¸åˆè§„æ¡æ¬¾æ€»ç»“", 2)
        non_compliant = [t for t in analysis if t['score'] < 70]
        
        if non_compliant:
            for term in non_compliant:
                para = doc.add_paragraph(f"åŸºå‡†æ¡æ¬¾: {term['benchmark_term'][:50]}...", style='List Number')
                para.runs[0].font.color.rgb = RGBColor(0xFF, 0x00, 0x00)  # çº¢è‰²
                doc.add_paragraph(f"åŒ¹é…åº¦: {term['score']}%")
                doc.add_paragraph("åˆ†æ:")
                doc.add_paragraph(term['analysis'], style='List Bullet')
                doc.add_paragraph("")
        else:
            doc.add_paragraph("æœªå‘ç°ä¸åˆè§„çš„æ¡æ¬¾")
        
        doc.add_page_break()
    
    # ä¿å­˜åˆ°å­—èŠ‚æµ
    buffer = io.BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    
    return buffer

# ä¸»å‡½æ•°
def main():
    # ä¸Šä¼ æ–‡ä»¶
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("åŸºå‡†æ–‡ä»¶")
        benchmark_file = st.file_uploader("ä¸Šä¼ åŸºå‡†æ–‡ä»¶ (PDFæˆ–DOCX)", type=['pdf', 'docx'], key='benchmark')
    
    with col2:
        st.subheader("å¯¹æ¯”æ–‡ä»¶")
        compare_files = st.file_uploader(
            "ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªå¯¹æ¯”æ–‡ä»¶ (PDFæˆ–DOCX)", 
            type=['pdf', 'docx'], 
            key='compare',
            accept_multiple_files=True
        )
    
    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆ†æ", disabled=not (benchmark_file and compare_files)):
        # å¤„ç†åŸºå‡†æ–‡ä»¶
        st.subheader("æ­£åœ¨å¤„ç†åŸºå‡†æ–‡ä»¶...")
        bench_type = benchmark_file.name.split('.')[-1].lower()
        bench_text = extract_text_from_file(benchmark_file, bench_type)
        
        if not bench_text:
            st.error("æ— æ³•ä»åŸºå‡†æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹")
            return
        
        # æ‹†åˆ†åŸºå‡†æ¡æ¬¾
        st.info("æ­£åœ¨æ‹†åˆ†åŸºå‡†æ¡æ¬¾...")
        bench_terms = split_chinese_terms(bench_text)
        st.success(f"æˆåŠŸæ‹†åˆ†å‡º {len(bench_terms)} æ¡åŸºå‡†æ¡æ¬¾")
        
        # æ˜¾ç¤ºéƒ¨åˆ†åŸºå‡†æ¡æ¬¾
        with st.expander("æŸ¥çœ‹éƒ¨åˆ†åŸºå‡†æ¡æ¬¾"):
            for i, term in enumerate(bench_terms[:5]):
                st.write(f"{i+1}. {term[:100]}...")
        
        # å¤„ç†æ¯ä¸ªå¯¹æ¯”æ–‡ä»¶
        compare_results = {}
        
        for compare_file in compare_files:
            st.subheader(f"æ­£åœ¨å¤„ç†å¯¹æ¯”æ–‡ä»¶: {compare_file.name}")
            file_type = compare_file.name.split('.')[-1].lower()
            compare_text = extract_text_from_file(compare_file, file_type)
            
            if not compare_text:
                st.warning(f"æ— æ³•ä» {compare_file.name} ä¸­æå–æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
                continue
            
            # æ‹†åˆ†å¯¹æ¯”æ¡æ¬¾
            st.info(f"æ­£åœ¨æ‹†åˆ† {compare_file.name} çš„æ¡æ¬¾...")
            compare_terms = split_chinese_terms(compare_text)
            st.success(f"æˆåŠŸæ‹†åˆ†å‡º {len(compare_terms)} æ¡å¯¹æ¯”æ¡æ¬¾")
            
            # åˆ†ææ¡æ¬¾åŒ¹é…æƒ…å†µ
            st.info(f"æ­£åœ¨åˆ†æ {compare_file.name} ä¸åŸºå‡†æ–‡ä»¶çš„åŒ¹é…æƒ…å†µ...")
            progress_bar = st.progress(0)
            analysis_results = []
            
            for i, bench_term in enumerate(bench_terms):
                # åˆ†æå½“å‰åŸºå‡†æ¡æ¬¾ä¸æ‰€æœ‰å¯¹æ¯”æ¡æ¬¾çš„åŒ¹é…åº¦
                score, analysis = analyze_terms_with_qwen(bench_term, compare_terms)
                
                # æ‰¾åˆ°æœ€åŒ¹é…çš„æ¡æ¬¾ï¼ˆç®€åŒ–å¤„ç†ï¼Œå®é™…åº”éå†æ‰€æœ‰å¯¹æ¯”æ¡æ¬¾ï¼‰
                matched_term = compare_terms[0] if compare_terms else "æ— å¯¹åº”æ¡æ¬¾"
                
                analysis_results.append({
                    "benchmark_term": bench_term,
                    "matched_term": matched_term,
                    "score": score,
                    "analysis": analysis
                })
                
                progress_bar.progress((i + 1) / len(bench_terms))
            
            compare_results[compare_file.name] = analysis_results
        
        # æ˜¾ç¤ºç»“æœ
        st.subheader("åˆ†æç»“æœ")
        tabs = st.tabs(list(compare_results.keys()))
        
        for tab, (file_name, results) in zip(tabs, compare_results.items()):
            with tab:
                # æ˜¾ç¤ºåŒ¹é…çš„æ¡æ¬¾
                st.subheader("å¯åŒ¹é…æ¡æ¬¾")
                matched = [r for r in results if r['score'] >= 70]
                
                if matched:
                    for i, res in enumerate(matched[:10]):  # åªæ˜¾ç¤ºå‰10æ¡
                        with st.expander(f"åŸºå‡†æ¡æ¬¾ {i+1} (åŒ¹é…åº¦: {res['score']}%)"):
                            st.write("**åŸºå‡†æ¡æ¬¾:**", res['benchmark_term'])
                            st.write("**åŒ¹é…æ¡æ¬¾:**", res['matched_term'])
                            st.write("**åˆ†æ:**", res['analysis'])
                    if len(matched) > 10:
                        st.info(f"å…± {len(matched)} æ¡åŒ¹é…æ¡æ¬¾ï¼Œæ˜¾ç¤ºå‰10æ¡")
                else:
                    st.info("æœªå‘ç°å¯åŒ¹é…çš„æ¡æ¬¾")
                
                # æ˜¾ç¤ºä¸åˆè§„æ¡æ¬¾
                st.subheader("ä¸åˆè§„æ¡æ¬¾")
                non_compliant = [r for r in results if r['score'] < 70]
                
                if non_compliant:
                    for i, res in enumerate(non_compliant[:10]):  # åªæ˜¾ç¤ºå‰10æ¡
                        with st.expander(f"åŸºå‡†æ¡æ¬¾ {i+1} (åŒ¹é…åº¦: {res['score']}%)"):
                            st.write("**åŸºå‡†æ¡æ¬¾:**", res['benchmark_term'])
                            st.write("**åŒ¹é…æ¡æ¬¾:**", res['matched_term'])
                            st.write("**åˆ†æ:**", res['analysis'])
                    if len(non_compliant) > 10:
                        st.info(f"å…± {len(non_compliant)} æ¡ä¸åˆè§„æ¡æ¬¾ï¼Œæ˜¾ç¤ºå‰10æ¡")
                else:
                    st.success("æœªå‘ç°ä¸åˆè§„çš„æ¡æ¬¾")
        
        # ç”ŸæˆæŠ¥å‘Š
        st.subheader("ç”ŸæˆæŠ¥å‘Š")
        if compare_results:
            report_buffer = generate_word_report(benchmark_file.name, compare_results)
            
            # æä¾›ä¸‹è½½
            b64 = base64.b64encode(report_buffer.read()).decode()
            href = f'<a href="data:application/octet-stream;base64,{b64}" download="æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š_{datetime.now().strftime("%Y%m%d")}.docx">ä¸‹è½½WordæŠ¥å‘Š</a>'
            st.markdown(href, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
    
