import streamlit as st
import docx
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
import re
import os
import tempfile
from datetime import datetime
from dashscope import Generation
import json
import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import numpy as np
import io

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ä¸­æ–‡æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ“„ ä¸­æ–‡æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”å·¥å…·")
st.write("ä¼˜åŒ–ä¸­æ–‡æ¡æ¬¾è§£æï¼Œæ”¯æŒPDFï¼ˆå«å›¾ç‰‡PDFï¼‰å’ŒWordæ–‡ä»¶ï¼Œç²¾ç¡®æ‹†åˆ†æ¡æ¬¾å¹¶è¿›è¡Œåˆè§„æ€§åˆ†æ")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.subheader("é…ç½®")
    
    # Qwen APIé…ç½®
    st.text("Qwenå¤§æ¨¡å‹é…ç½®")
    qwen_api_key = st.text_input("é˜¿é‡Œäº‘DashScope APIå¯†é’¥", type="password")
    if qwen_api_key:
        os.environ["DASHSCOPE_API_KEY"] = qwen_api_key
    
    # OCRé…ç½®
    st.text("\nOCRé…ç½®")
    tesseract_path = st.text_input(
        "Tesseract OCRè·¯å¾„", 
        value=r"C:\Program Files\Tesseract-OCR\tesseract.exe" if os.name == 'nt' else "/usr/bin/tesseract"
    )
    pytesseract.pytesseract.tesseract_cmd = tesseract_path
    
    st.info("æç¤ºï¼šå¤„ç†æ‰«æä»¶PDFéœ€è¦å®‰è£…Tesseract OCRåŠä¸­æ–‡è¯­è¨€åŒ…")

# ------------------------------
# ä¸­æ–‡æ–‡æœ¬å¤„ç†ä¼˜åŒ–å‡½æ•°
# ------------------------------

def is_chinese_char(c):
    """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦"""
    return '\u4e00' <= c <= '\u9fff'

def clean_chinese_text(text):
    """æ¸…ç†ä¸­æ–‡æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºè¡Œå’Œç©ºæ ¼"""
    # å¤„ç†ä¸­æ–‡æ ‡ç‚¹ç¬¦å·å‰åçš„ç©ºæ ¼
    text = re.sub(r'(\s+)([ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ,.;:!?])', r'\2', text)
    text = re.sub(r'([ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ,.;:!?])(\s+)', r'\1', text)
    
    # åˆå¹¶è¿‡å¤šç©ºè¡Œ
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    return '\n'.join(lines)

# ------------------------------
# PDFè§£æä¼˜åŒ–ï¼ˆé’ˆå¯¹ä¸­æ–‡ï¼‰
# ------------------------------

def extract_text_from_pdf(pdf_path):
    """ä»PDFæå–æ–‡æœ¬ï¼Œä¼˜åŒ–ä¸­æ–‡å¤„ç†"""
    doc = fitz.open(pdf_path)
    full_text = []
    
    for page_num, page in enumerate(doc):
        # å°è¯•ç›´æ¥æå–æ–‡æœ¬
        page_text = page.get_text("text")
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºæ‰«æé¡µ
        chinese_chars = sum(1 for c in page_text if is_chinese_char(c))
        if len(page_text.strip()) < 50 and chinese_chars < 10:
            # æ‰«æé¡µï¼Œä½¿ç”¨OCR
            st.warning(f"ç¬¬{page_num+1}é¡µå¯èƒ½ä¸ºå›¾ç‰‡ï¼Œå°†ä½¿ç”¨OCRæå–æ–‡æœ¬")
            pix = page.get_pixmap()
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # é¢„å¤„ç†å›¾åƒä»¥æé«˜OCRç²¾åº¦ï¼ˆé’ˆå¯¹ä¸­æ–‡ï¼‰
            img_np = np.array(img)
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = np.mean(img_np, axis=2).astype(np.uint8)
            # ç®€å•äºŒå€¼åŒ–å¤„ç†
            threshold = 150
            binary = (gray > threshold) * 255
            img_processed = Image.fromarray(binary.astype(np.uint8))
            
            # è¯†åˆ«ä¸­æ–‡æ–‡æœ¬
            ocr_text = pytesseract.image_to_string(
                img_processed, 
                lang="chi_sim+eng",  # ä¸­è‹±æ–‡æ··åˆè¯†åˆ«
                config='--psm 6'  # å‡è®¾ä¸ºå•ä¸€å‡åŒ€æ–‡æœ¬å—
            )
            full_text.append(ocr_text)
        else:
            # æ­£å¸¸æ–‡æœ¬é¡µï¼Œæ¸…ç†åæ·»åŠ 
            cleaned_text = clean_chinese_text(page_text)
            full_text.append(cleaned_text)
    
    return '\n'.join(full_text)

# ------------------------------
# æ¡æ¬¾æ‹†åˆ†ä¼˜åŒ–ï¼ˆé’ˆå¯¹ä¸­æ–‡æ¡æ¬¾ç‰¹ç‚¹ï¼‰
# ------------------------------

def split_chinese_terms(text):
    """ä¼˜åŒ–ä¸­æ–‡æ¡æ¬¾æ‹†åˆ†ï¼Œå¤„ç†å„ç§å¸¸è§çš„æ¡æ¬¾ç¼–å·æ ¼å¼"""
    # ä¸­æ–‡æ¡æ¬¾å¸¸è§ç¼–å·æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
    # åŒ¹é…ï¼š1. ã€(1)ã€ä¸€ã€1.1 ã€1.1.1ã€ç¬¬ä¸€æ¡ã€ç¬¬ä¸€æ¬¾ç­‰æ ¼å¼
    patterns = [
        r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+[æ¡æ¬¾é¡¹ç‚¹])(\.?\s?)',  # ç¬¬ä¸€æ¡ã€ç¬¬ä¸€æ¬¾
        r'^(\d+)\.\s',  # 1. 
        r'^(\d+\.\d+)\.\s',  # 1.1.
        r'^(\(\d+\))\s',  # (1)
        r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\.\s',  # ä¸€. 
        r'^(\d+\.\d+\.\d+)\s'  # 1.1.1 
    ]
    
    terms = []
    current_term = []
    current_number = None
    
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ¡æ¬¾å¼€å¤´
        matched = False
        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                # å¦‚æœæœ‰å½“å‰æ¡æ¬¾ï¼Œå…ˆä¿å­˜
                if current_term:
                    terms.append({
                        'number': current_number,
                        'content': ' '.join(current_term).strip()
                    })
                
                # å¼€å§‹æ–°æ¡æ¬¾
                current_number = match.group(1)
                current_term = [line[len(match.group(0)):].strip()]
                matched = True
                break
        
        if not matched and current_term:
            # ä¸æ˜¯æ–°æ¡æ¬¾ï¼Œæ·»åŠ åˆ°å½“å‰æ¡æ¬¾
            current_term.append(line)
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ¡æ¬¾
    if current_term:
        terms.append({
            'number': current_number,
            'content': ' '.join(current_term).strip()
        })
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ¡æ¬¾æ ¼å¼ï¼Œä½¿ç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½æ‹†åˆ†
    if len(terms) <= 1:
        st.info("æ£€æµ‹åˆ°æ¡æ¬¾æ ¼å¼å¤æ‚ï¼Œå°†ä½¿ç”¨Qwenå¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½æ‹†åˆ†")
        return split_terms_with_qwen(text)
    
    return terms

def split_terms_with_qwen(text):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹æ™ºèƒ½æ‹†åˆ†ä¸­æ–‡æ¡æ¬¾"""
    if not qwen_api_key:
        st.error("è¯·å…ˆé…ç½®Qwen APIå¯†é’¥ä»¥å¤„ç†å¤æ‚æ¡æ¬¾")
        return []
    
    prompt = f"""è¯·å¸®æˆ‘ä»ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ä¸­æå–æ¡æ¬¾ï¼Œæ¯æ¡æ¡æ¬¾ä½œä¸ºä¸€ä¸ªç‹¬ç«‹é¡¹ã€‚
    æ¡æ¬¾é€šå¸¸ä»¥ä»¥ä¸‹å½¢å¼å¼€å¤´ï¼š
    - æ•°å­—ç¼–å·ï¼š1. ã€1.1 ã€(1) ç­‰
    - ä¸­æ–‡ç¼–å·ï¼šç¬¬ä¸€æ¡ã€ç¬¬ä¸€æ¬¾ã€ä¸€ã€ä¸€ç­‰
    
    è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯åŒ…å«"number"ï¼ˆæ¡æ¬¾ç¼–å·ï¼‰å’Œ"content"ï¼ˆæ¡æ¬¾å†…å®¹ï¼‰çš„å¯¹è±¡ã€‚
    ç¡®ä¿å‡†ç¡®æ‹†åˆ†ï¼Œä¿æŒæ¡æ¬¾çš„å®Œæ•´æ€§å’Œç‹¬ç«‹æ€§ã€‚
    
    æ–‡æœ¬å†…å®¹ï¼š
    {text[:3000]}
    """
    
    try:
        response = Generation.call(
            model="qwen-plus",
            prompt=prompt,
            result_format="json"
        )
        
        if response.status_code == 200:
            try:
                terms = json.loads(response.output.text)
                return terms if isinstance(terms, list) else []
            except json.JSONDecodeError:
                st.warning("Qwenè¿”å›ç»“æœæ ¼å¼ä¸æ­£ç¡®ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ‹†åˆ†æ–¹æ³•")
                return split_chinese_terms(text)
        else:
            st.error(f"Qwen APIè°ƒç”¨å¤±è´¥: {response.message}")
            return split_chinese_terms(text)
    except Exception as e:
        st.error(f"æ¡æ¬¾æ‹†åˆ†å‡ºé”™: {str(e)}")
        return split_chinese_terms(text)

# ------------------------------
# é€šç”¨æ–‡ä»¶å¤„ç†å‡½æ•°
# ------------------------------

def extract_text_from_docx(file_path):
    """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
    doc = docx.Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        if para.text.strip():
            full_text.append(para.text)
    return clean_chinese_text('\n'.join(full_text))

def extract_text_from_file(file, file_type):
    """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=f'.{file_type}') as f:
        f.write(file.getbuffer())
        temp_path = f.name
    
    try:
        if file_type == 'pdf':
            text = extract_text_from_pdf(temp_path)
        elif file_type in ['docx', 'doc']:  # ç®€å•æ”¯æŒdocæ ¼å¼
            text = extract_text_from_docx(temp_path)
        else:
            text = ""
            st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
    finally:
        os.unlink(temp_path)
    
    return text

# ------------------------------
# æ¡æ¬¾åŒ¹é…ä¸åˆ†æ
# ------------------------------

def analyze_compliance(benchmark_terms, compare_terms):
    """åˆ†ææ¡æ¬¾åˆè§„æ€§"""
    if not qwen_api_key:
        st.error("è¯·é…ç½®Qwen APIå¯†é’¥ä»¥è¿›è¡Œåˆè§„æ€§åˆ†æ")
        return []
    
    results = []
    
    # æ˜¾ç¤ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    total = len(benchmark_terms)
    
    for i, bench_term in enumerate(benchmark_terms):
        progress_bar.progress((i + 1) / total)
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹å¯¹æ¯”æ¡æ¬¾ä¸åŸºå‡†æ¡æ¬¾çš„åˆè§„æ€§ï¼š
        
        åŸºå‡†æ¡æ¬¾[{bench_term.get('number', '')}]ï¼š
        {bench_term.get('content', '')}
        
        å¯¹æ¯”æ¡æ¬¾åˆ—è¡¨ï¼š
        {json.dumps(compare_terms, ensure_ascii=False, indent=2)}
        
        è¯·æ‰¾å‡ºæœ€åŒ¹é…çš„å¯¹æ¯”æ¡æ¬¾ï¼Œå¹¶åˆ†æï¼š
        1. æ˜¯å¦åˆè§„ï¼ˆç›¸ä¼¼åº¦æ˜¯å¦è¾¾åˆ°80%ä»¥ä¸Šï¼‰
        2. ä¸»è¦å·®å¼‚ç‚¹ï¼ˆå¦‚æœä¸åˆè§„ï¼‰
        3. åŒ¹é…çš„æ¡æ¬¾ç¼–å·
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ï¼š
        - benchmark_number: åŸºå‡†æ¡æ¬¾ç¼–å·
        - matched_number: åŒ¹é…çš„å¯¹æ¯”æ¡æ¬¾ç¼–å·ï¼ˆå¦‚æ— åˆ™ä¸ºnullï¼‰
        - is_compliant: æ˜¯å¦åˆè§„ï¼ˆtrue/falseï¼‰
        - similarity: ç›¸ä¼¼åº¦ï¼ˆ0-100ï¼‰
        - differences: å·®å¼‚æè¿°ï¼ˆå¦‚ä¸åˆè§„ï¼‰
        """
        
        try:
            response = Generation.call(
                model="qwen-plus",
                prompt=prompt,
                result_format="json"
            )
            
            if response.status_code == 200:
                analysis = json.loads(response.output.text)
                results.append({
                    'benchmark': bench_term,
                    'analysis': analysis
                })
            else:
                st.warning(f"åˆ†ææ¡æ¬¾[{bench_term.get('number')}]å¤±è´¥: {response.message}")
        except Exception as e:
            st.warning(f"åˆ†ææ¡æ¬¾[{bench_term.get('number')}]å‡ºé”™: {str(e)}")
    
    progress_bar.empty()
    return results

# ------------------------------
# æŠ¥å‘Šç”Ÿæˆ
# ------------------------------

def generate_report(benchmark_name, compare_files_results):
    """ç”ŸæˆWordæŠ¥å‘Š"""
    doc = docx.Document()
    
    # æ ‡é¢˜
    title = doc.add_heading("æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # åŸºæœ¬ä¿¡æ¯
    doc.add_paragraph(f"åŸºå‡†æ–‡ä»¶: {benchmark_name}")
    doc.add_paragraph(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    doc.add_paragraph("")
    
    # ç›®å½•
    doc.add_heading("ç›®å½•", 1)
    for i, (file_name, results) in enumerate(compare_files_results.items(), 1):
        doc.add_paragraph(f"{i}. {file_name}", style='List Number')
    doc.add_paragraph("")
    
    # æ¯ä¸ªå¯¹æ¯”æ–‡ä»¶çš„åˆ†æç»“æœ
    for file_name, results in compare_files_results.items():
        doc.add_heading(f"æ–‡ä»¶: {file_name}", 1)
        
        # åˆè§„æ¡æ¬¾
        doc.add_heading("1. åˆè§„æ¡æ¬¾", 2)
        compliant_terms = [r for r in results if r['analysis'].get('is_compliant', False)]
        
        if compliant_terms:
            for term in compliant_terms:
                p = doc.add_paragraph()
                p.add_run(f"åŸºå‡†æ¡æ¬¾[{term['benchmark']['number']}]: ").bold = True
                p.add_run(term['benchmark']['content'])
                
                p = doc.add_paragraph()
                p.add_run(f"åŒ¹é…æ¡æ¬¾[{term['analysis']['matched_number']}]: ").bold = True
                p.add_run(f"(ç›¸ä¼¼åº¦: {term['analysis']['similarity']}%)")
                doc.add_paragraph("")
        else:
            doc.add_paragraph("æ— åˆè§„æ¡æ¬¾")
        
        # ä¸åˆè§„æ¡æ¬¾
        doc.add_heading("2. ä¸åˆè§„æ¡æ¬¾", 2)
        non_compliant_terms = [r for r in results if not r['analysis'].get('is_compliant', False)]
        
        if non_compliant_terms:
            for term in non_compliant_terms:
                p = doc.add_paragraph()
                p.add_run(f"åŸºå‡†æ¡æ¬¾[{term['benchmark']['number']}]: ").bold = True
                p.add_run(term['benchmark']['content'])
                
                p = doc.add_paragraph()
                p.add_run("å·®å¼‚åˆ†æ: ").bold = True
                p.add_run(term['analysis'].get('differences', 'æ— è¯¦ç»†åˆ†æ'))
                doc.add_paragraph("")
        else:
            doc.add_paragraph("æ— ä¸åˆè§„æ¡æ¬¾")
        
        doc.add_page_break()
    
    # ä¿å­˜åˆ°å†…å­˜
    buffer = io.BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    
    return buffer

# ------------------------------
# ä¸»ç¨‹åº
# ------------------------------

def main():
    # æ–‡ä»¶ä¸Šä¼ 
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("åŸºå‡†æ–‡ä»¶")
        benchmark_file = st.file_uploader("ä¸Šä¼ åŸºå‡†æ¡æ¬¾æ–‡ä»¶", type=['pdf', 'docx', 'doc'], key='benchmark')
    
    with col2:
        st.subheader("å¯¹æ¯”æ–‡ä»¶")
        compare_files = st.file_uploader(
            "ä¸Šä¼ éœ€è¦å¯¹æ¯”çš„æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰", 
            type=['pdf', 'docx', 'doc'], 
            key='compare',
            accept_multiple_files=True
        )
    
    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆ†æ", disabled=not (benchmark_file and compare_files)):
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
            # å¤„ç†åŸºå‡†æ–‡ä»¶
            st.info(f"æ­£åœ¨è§£æåŸºå‡†æ–‡ä»¶: {benchmark_file.name}")
            bench_type = benchmark_file.name.split('.')[-1].lower()
            bench_text = extract_text_from_file(benchmark_file, bench_type)
            
            # æ‹†åˆ†åŸºå‡†æ¡æ¬¾
            st.info("æ­£åœ¨æ‹†åˆ†åŸºå‡†æ¡æ¬¾...")
            benchmark_terms = split_chinese_terms(bench_text)
            st.success(f"æˆåŠŸæ‹†åˆ†åŸºå‡†æ¡æ¬¾: {len(benchmark_terms)}æ¡")
            
            # å¤„ç†å¯¹æ¯”æ–‡ä»¶
            compare_files_results = {}
            
            for compare_file in compare_files:
                st.info(f"æ­£åœ¨å¤„ç†å¯¹æ¯”æ–‡ä»¶: {compare_file.name}")
                file_type = compare_file.name.split('.')[-1].lower()
                file_text = extract_text_from_file(compare_file, file_type)
                
                # æ‹†åˆ†å¯¹æ¯”æ¡æ¬¾
                st.info(f"æ­£åœ¨æ‹†åˆ†{compare_file.name}çš„æ¡æ¬¾...")
                compare_terms = split_chinese_terms(file_text)
                st.success(f"æˆåŠŸæ‹†åˆ†{compare_file.name}çš„æ¡æ¬¾: {len(compare_terms)}æ¡")
                
                # åˆ†æåˆè§„æ€§
                st.info(f"æ­£åœ¨åˆ†æ{compare_file.name}çš„åˆè§„æ€§...")
                results = analyze_compliance(benchmark_terms, compare_terms)
                compare_files_results[compare_file.name] = results
            
            # å±•ç¤ºç»“æœ
            st.success("åˆ†æå®Œæˆï¼")
            
            # ä½¿ç”¨æ ‡ç­¾é¡µå±•ç¤ºæ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
            tabs = st.tabs(list(compare_files_results.keys()))
            
            for tab, (file_name, results) in zip(tabs, compare_files_results.items()):
                with tab:
                    # åˆè§„æ¡æ¬¾
                    st.subheader("âœ… å¯åŒ¹é…çš„æ¡æ¬¾")
                    compliant = [r for r in results if r['analysis'].get('is_compliant', False)]
                    
                    if compliant:
                        for item in compliant:
                            with st.expander(f"åŸºå‡†æ¡æ¬¾[{item['benchmark']['number']}] ä¸ å¯¹æ¯”æ¡æ¬¾[{item['analysis']['matched_number']}] (ç›¸ä¼¼åº¦: {item['analysis']['similarity']}%)"):
                                col_bench, col_compare = st.columns(2)
                                with col_bench:
                                    st.write("**åŸºå‡†æ¡æ¬¾å†…å®¹ï¼š**")
                                    st.write(item['benchmark']['content'])
                                with col_compare:
                                    st.write("**å¯¹æ¯”æ¡æ¬¾å†…å®¹ï¼š**")
                                    # è¿™é‡Œéœ€è¦æ ¹æ®matched_numberæ‰¾åˆ°å¯¹åº”çš„æ¡æ¬¾å†…å®¹
                                    st.write("(å¯¹æ¯”æ¡æ¬¾å†…å®¹å°†åœ¨æ­¤æ˜¾ç¤º)")
                    else:
                        st.info("æ²¡æœ‰æ‰¾åˆ°å¯åŒ¹é…çš„æ¡æ¬¾")
                    
                    # ä¸åˆè§„æ¡æ¬¾
                    st.subheader("âŒ ä¸åˆè§„æ¡æ¬¾æ€»ç»“")
                    non_compliant = [r for r in results if not r['analysis'].get('is_compliant', False)]
                    
                    if non_compliant:
                        for item in non_compliant:
                            with st.expander(f"åŸºå‡†æ¡æ¬¾[{item['benchmark']['number']}]"):
                                st.write("**åŸºå‡†æ¡æ¬¾å†…å®¹ï¼š**")
                                st.write(item['benchmark']['content'])
                                st.write("**å·®å¼‚åˆ†æï¼š**")
                                st.write(item['analysis'].get('differences', 'æ— è¯¦ç»†åˆ†æ'))
                    else:
                        st.info("æ‰€æœ‰æ¡æ¬¾å‡åˆè§„")
            
            # ç”ŸæˆæŠ¥å‘Š
            st.subheader("ğŸ“¥ ç”ŸæˆæŠ¥å‘Š")
            report_buffer = generate_report(benchmark_file.name, compare_files_results)
            st.download_button(
                label="ä¸‹è½½WordæŠ¥å‘Š",
                data=report_buffer,
                file_name=f"æ¡æ¬¾åˆè§„æ€§å¯¹æ¯”æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )

if __name__ == "__main__":
    main()
    
